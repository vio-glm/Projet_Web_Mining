{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c42a4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re # re = biblioth√®que de gestion des expressions r√©guli√®res (regular expressions = Regex) en Python\n",
    "# scanne un texte (ex : full_path) √† la recherche de motifs sp√©cifiques\n",
    "# fonctions : \n",
    "# re.search() : recherche un motif dans une cha√Æne de caract√®res et renvoie un objet de correspondance si trouv√©, sinon None\n",
    "# re.match() : v√©rifie si le d√©but d'une cha√Æne de caract√®res correspond √† un motif donn√©\n",
    "# re.findall() : trouve toutes les occurrences d'un motif dans une cha√Æne de caract√®res et les renvoie sous forme de liste\n",
    "    # ex : print(len(re.findall(r'\\bch\\b', texte))) affiche le nombre de fois que le mot \"ch\" appara√Æt dans le texte\n",
    "# re.sub() ...\n",
    "# re.compile() ...\n",
    "# re.split() ...\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10ab7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION DES FILTRES ---\n",
    "\n",
    "# Utilisation des expressions r√©guli√®res (regex) pour filtrer les URLs non d√©sir√©es\n",
    "\n",
    "# Filtres pour d√©tecter les pages commerciales inutiles\n",
    "# \"r\" = regex = raw string = cha√Æne brute, interpr√®te les backslashes litt√©ralement\n",
    "FILTRES_COMMERCIAUX = [\n",
    "    r'/products?/',        # ? = dernier caract√®re pas obligatoire donc /products/ ou /product/\n",
    "    r'/shop/',             # /shop/categorie\n",
    "    r'/store/',            # /store/item\n",
    "    r'/item\\d+',           # \\d+ : \"d\" √©quivaut √† [0-9] et \"+\" les prend par paquet tant qu'ils se suivent, ex : item\\12345\n",
    "                           # \\w √©quivaut √† [a-z, A-Z, 0-9 et _] (lettres, chiffres, underscore) \n",
    "    r'/cart',              # /cart (panier)\n",
    "    r'/checkout',          # /checkout (paiement)\n",
    "    r'/account',           # /account (compte utilisateur)\n",
    "    r'\\?.*(id=|sku=|price=)'] # \\? permet de garder le ? comme quand on √©crit un texte entre \"\" \n",
    "                           #Les param√®tres d'URL commerciaux commencent par ?\n",
    "                           #Le .* permet de dire \"n'importe quoi entre les deux\"\n",
    "                           #et on cherche id= ou sku= ou price= car | signifie \"ou\"\n",
    "\n",
    "# Filtres pour d√©tecter les pages techniques inutiles\n",
    "FILTRES_INNUTILES = [\n",
    "    r'/search',            # \\search (r√©sultats de recherche)\n",
    "    r'/login',             # \\login (connexion)\n",
    "    r'/register',          # \\register (inscription)\n",
    "    r'/password',          # \\password (mot de passe oubli√©)\n",
    "    r'#',                  # √©vite les doublons d'url contenant plusieurs chapitres par exemple\n",
    "    r'\\.(pdf|jpg|png|zip)$'] # supprime les fichiers non HTML\n",
    "                           # \\. force √† chercher le caract√®re \".\" et veut dire \"n'importe quoi\" \n",
    "                           # | signifie \"ou\" \n",
    "                           # $ signifie \"√† la fin de l'URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "694b61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def useful_url(url):\n",
    "# normaliser les liens pour pouvoir utiliser les regex dessus et ainsi trier \n",
    "# retourne True si l'URL semble √™tre du contenu utile, False sinon\n",
    "    \n",
    "    parsed = urlparse(url) # urlparse est une fonction qui d√©compose l'URL en parties (chemin + param√®tres)\n",
    "    path = parsed.path.lower() # chemin de l'URL en minuscules\n",
    "    query = parsed.query.lower() # param√®tres de l'URL en minuscules\n",
    "    full_path = path + \"?\" + query \n",
    "    # exemple : https://www.example.com/products/item?id=456&ref=google\n",
    "    # path = /products/item\n",
    "    # query = id=456&ref=google (ce qui vient apr√®s le \"?\")\n",
    "\n",
    "    # 1. V√©rifie si c'est un produit :\n",
    "    for filtre in FILTRES_COMMERCIAUX:\n",
    "        if re.search(filtre, full_path): # cherche la correspondance avec un filtre commercial dans le nouvel url \"full_path\"\n",
    "        # pour re voir dans les imports\n",
    "            return False\n",
    "            \n",
    "    # 2. V√©rifie si c'est une page technique inutile : \n",
    "    for filtre in FILTRES_INNUTILES:\n",
    "        if re.search(filtre, full_path): # cherche la correspondance avec un filtre inutile dans le nouvel url \"full_path\"\n",
    "            return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_by_levels(start_url, max_levels=3):\n",
    "# crawl le site niveau par niveau (BFS)\n",
    "# max_levels=3 signifie : Accueil (0) -> Liens accueil (1) -> Liens de niveau 1 (2)\n",
    "\n",
    "    domain = urlparse(start_url).netloc\n",
    "    print(f\"üöÄ D√©marrage du crawl sur : {domain}\")\n",
    "    print(f\"üéØ Objectif : {max_levels} niveaux max\")\n",
    "\n",
    "    visited = set() #    # Set pour stocker les URLs d√©j√† vues (√©vite les boucles infinies)\n",
    "    texts = [] #    # Liste qui contiendra les r√©sultats finaux (url, texte, niveau)\n",
    "    \n",
    "    # File d'attente pour le niveau actuel (commence avec l'URL de d√©part)\n",
    "    current_level_queue = deque([start_url]) # # File d'attente (Queue) initiale contenant juste l'URL de d√©part\n",
    "    # deque est optimis√© pour ajouter/retirer des √©l√©ments rapidement\n",
    "    \n",
    "    for level in range(max_levels):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìç NIVEAU {level} : {len(current_level_queue)} pages √† explorer\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Si la file est vide, on arr√™te tout (plus rien √† explorer)\n",
    "        if not current_level_queue:\n",
    "            print(\"üõë Plus aucun lien √† explorer √† ce niveau.\")\n",
    "            break\n",
    "            \n",
    "        # Pr√©paration de la file pour le PROCHAIN niveau\n",
    "        # Nouvelle file vide pour stocker les enfants des pages actuelles\n",
    "        next_level_queue = deque()\n",
    "        links_found_at_this_level = 0\n",
    "        \n",
    "        # On vide la file du niveau actuel\n",
    "        # Tant qu'il reste des pages dans la file du niveau courant...\n",
    "        while current_level_queue:\n",
    "            # On prend la premi√®re URL de la file (FIFO: First In, First Out)\n",
    "            url = current_level_queue.popleft()\n",
    "            # S√©curit√© anti-boucle : si on l'a d√©j√† vue, on passe    \n",
    "            if url in visited:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Petite pause pour ne pas DDOS le site\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                print(f\"  Example ({len(visited)} total): {url}\")\n",
    "            \n",
    "                # T√©l√©chargement de la page (timeout 10s pour ne pas bloquer)    \n",
    "                response = requests.get(url, timeout=10)\n",
    "                # Si erreur (404, 500, etc.), on ignore et on passe √† la suivante\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"    ‚ö†Ô∏è Status {response.status_code} - Ignor√©\")\n",
    "                    continue\n",
    "                \n",
    "                # Analyse du HTML avec BeautifulSoup\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                # Extraction du texte\n",
    "                text_content = soup.get_text(separator=\" \").strip()\n",
    "\n",
    "                # Stockage du r√©sultat\n",
    "                texts.append({\n",
    "                    \"url\": url, \n",
    "                    \"text\": text_content[:200] + \"...\", # On stocke les 200 premiers caract√®res (test)\n",
    "                    \"level\": level})\n",
    "                \n",
    "                # Marquer comme visit√© pour ne plus jamais la refaire\n",
    "                visited.add(url)\n",
    "                \n",
    "                # Recherche des liens pour le niveau suivant\n",
    "                # On ne cherche des liens que si on n'est pas au dernier niveau demand√©\n",
    "                if level < max_levels - 1:\n",
    "                    # Trouve toutes les balises <a href=\"...\">\n",
    "                    page_links = soup.find_all(\"a\", href=True)\n",
    "                    for a in page_links:\n",
    "                        # Convertit lien relatif (/blog) en absolu (https://site.com/blog)\n",
    "                        link = urljoin(url, a[\"href\"])\n",
    "                        parsed_link = urlparse(link)\n",
    "                        \n",
    "                        # Conditions strictes pour ajouter √† la suite\n",
    "                        if (\"\"\"parsed_link.netloc == domain and\"\"\" # reste sur le m√™me domaine\n",
    "                            link not in visited and                # pas d√©j√† visit√©\n",
    "                            link not in current_level_queue and    # pas dans la file actuelle\n",
    "                            link not in next_level_queue and       # pas d√©j√† pr√©vu pour la suite\n",
    "                            not parsed_link.fragment and           # pas d'ancre (#)\n",
    "                            useful_url(link)):                     # appelle fonction filtres pour tri\n",
    "                            \n",
    "                            # Ajoute √† la file du PROCHAIN niveau\n",
    "                            next_level_queue.append(link)\n",
    "                            links_found_at_this_level += 1\n",
    "\n",
    "            except Exception as e:#en cas de crash, affiche l'erreur et on continue\n",
    "                print(f\"    ‚ùå Erreur sur {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"\\n‚úÖ FIN NIVEAU {level}\")\n",
    "        print(f\"üìä Liens trouv√©s pour le niveau {level+1} : {links_found_at_this_level}\")\n",
    "        \n",
    "        # La file du prochain niveau devient la file courante pour le tour de boucle suivant\n",
    "\n",
    "        current_level_queue = next_level_queue\n",
    "\n",
    "    print(f\"\\nüéâ CRAWL TERMIN√â : {len(texts)} pages r√©cup√©r√©es au total.\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "503663d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
