{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171c8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "pip install gensim # pour le doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b86ee4",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8274a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import csv\n",
    "from dateutil import parser\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0739d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')   \n",
    "nltk.download('omw-1.4')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc56c7a",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf6ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF COLORS ---\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF STOPWORDS ---\n",
    "\n",
    "custom_stopwords = {\n",
    "    # High-frequency but uniteresting\n",
    "    \"one\", \"two\", \"first\", \"second\", \"new\", \"more\", \"most\", \"many\", \"much\",\n",
    "    \"even\", \"still\", \"just\", \"really\", \"very\", \"well\", \"good\", \"great\", \"better\",\n",
    "    \"every\", \"each\", \"some\", \"any\", \"another\", \"few\", \"several\", \"also\", \"like\", \"anti\", \"something\",\n",
    "\n",
    "    # Adverbes\n",
    "    \"often\", \"sometimes\", \"usually\", \"typically\", \"generally\", \"likely\", \"probably\",\n",
    "    \"maybe\", \"perhaps\", \"always\", \"never\", \"already\", \"again\", \"soon\", \"now\", \"then\",\n",
    "    \"various\", \"numerous\", \"multiple\", \"today\", \"yesterday\", \"tomorrow\",\n",
    "    \"currently\", \"recently\", \"eventually\", \"rarely\", \"occasionally\",\n",
    "\n",
    "    # Generic verbs (not in standard stopwords)\n",
    "    \"get\", \"got\", \"make\", \"made\", \"take\", \"took\", \"use\", \"used\", \"using\", \"help\", \"helps\",\n",
    "    \"see\", \"seen\", \"know\", \"known\", \"show\", \"shows\", \"say\", \"says\", \"said\", \"find\", \"found\",\n",
    "\n",
    "    # Generic nouns with low semantic value \n",
    "    \"thing\", \"things\", \"stuff\", \"kind\", \"kinds\", \"type\", \"types\", \"way\", \"ways\",\n",
    "    \"part\", \"parts\", \"area\", \"areas\", \"aspect\", \"aspects\", \"place\", \"places\",\n",
    "    \"lot\", \"bit\", \"time\", \"day\", \"week\", \"month\", \"year\",\n",
    "    \"someone\", \"anyone\", \"everyone\", \"nobody\",\n",
    "\n",
    "    # Mots de liaison ou de structure\n",
    "    \"however\", \"therefore\", \"thus\", \"though\", \"although\", \"because\", \"while\", \"since\",\n",
    "    \"before\", \"after\", \"during\", \"when\", \"where\", \"whose\", \"which\", \"who\", \"whom\",\n",
    "\n",
    "    # Names\n",
    "    \"sarah\", \"regan\", \"amanda\", \"shayne\", \"kelly\", \"gansalves\", \"abby\", \"moore\", \"jason\", \"wachob\", \"julie\", \n",
    "    \"nguyen\", \"braelyn\", \"wood\", \"alisa\", \"cowell\", \"alexandra\", \"engler\", \"ava\", \"durgin\", \"carleigh\", \"ferrante\", \n",
    "    \"hannah\", \"margaret\", \"allen\", \"molly\", \"knudsen\", \"emma\", \"engler\", \"devon\", \"barrow\", \"eliza\", \"sullivan\", \n",
    "    \"loewe\", \"jamie\", \"scheinder\", \"frye\", \"lindsay\", \"boyers\", \"kelsea\", \"samson\", \"editorial\", \"jamie\", \"scheinder\", \"ryan\", \n",
    "    \"brady\", \"megan\", \"falk\", \"julia\", \"guerra\", \"stephanie\", \"eckelkamp\", \"india\", \"edwards\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7363dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF DATE EXTRACTION ---\n",
    "\n",
    "date_patterns = [\n",
    "        # Detect dates like : \"September 17, 2024\" and \"Sep 17, 2024\"\n",
    "        r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|'\n",
    "        r'Jul(?:y)?|Aug(?:ust)?|Sep(?:t(?:ember)?)?|Oct(?:ober)?|Nov(?:ember)?|'\n",
    "        r'Dec(?:ember)?)\\s\\d{1,2},\\s\\d{4}\\b',\n",
    "\n",
    "        # Detect dates like : \"September 17 2024\" \n",
    "        r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|'\n",
    "        r'Jul(?:y)?|Aug(?:ust)?|Sep(?:t(?:ember)?)?|Oct(?:ober)?|Nov(?:ember)?|'\n",
    "        r'Dec(?:ember)?)\\s\\d{1,2}\\s\\d{4}\\b',\n",
    "\n",
    "        # Detect dates like : \"17 September 2024\"\n",
    "        r'\\b\\d{1,2}\\s(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|'\n",
    "        r'Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:t(?:ember)?)?|Oct(?:ober)?|'\n",
    "        r'Nov(?:ember)?|Dec(?:ember)?)\\s\\d{4}\\b',\n",
    "\n",
    "        # Detect dates like : \"2024-09-17\" (ISO)\n",
    "        r'\\b\\d{4}-\\d{2}-\\d{2}\\b',\n",
    "\n",
    "        # Detect dates like : \"09/17/2024\" and \"9/17/24\" \n",
    "        r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47d6d4",
   "metadata": {},
   "source": [
    "## Faut trouver un titre pour clean, norm, tok "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f8f9e",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efe1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(source_name): \n",
    "    # Create all files from a source using the name specified in the source cell [21]\n",
    "    \n",
    "    directory_name = \"data\"\n",
    "    \n",
    "    if not os.path.exists(directory_name): # Checks if the 'data' folder exists\n",
    "        os.makedirs(directory_name) # Setup the output directory\n",
    "        print(f\"{GREEN}Created directory : {directory_name}{RESET}\")\n",
    "    \n",
    "    # We assume that source_name contains no spaces. If changed, add : source_name = source_name.replace(\" \", \"_\")\n",
    "\n",
    "    return {\"raw\":          os.path.join(directory_name, f\"{source_name}_raw_html.csv\"),\n",
    "            'content_date': os.path.join(directory_name, f\"{source_name}_content_date.csv\"),\n",
    "            \"corpus\":       os.path.join(directory_name, f\"{source_name}_corpus.csv\"),\n",
    "            \"cleaned\":      os.path.join(directory_name, f\"{source_name}_cleaned.csv\"),\n",
    "            \"norm\":         os.path.join(directory_name, f\"{source_name}_corpus_norm.csv\"),\n",
    "            \"tokenized\":    os.path.join(directory_name, f\"{source_name}_corpus_tokenized.csv\"),\n",
    "            \"nodes\" :       os.path.join(directory_name, f\"{source_name}_nodes.csv\"),\n",
    "            \"edges\" :       os.path.join(directory_name, f\"{source_name}_edges.csv\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad17bd0",
   "metadata": {},
   "source": [
    "Date extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f31acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_html(html):\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE) # Find all date strings in the text that match one of the date pattern\n",
    "\n",
    "        for match in matches:  \n",
    "            try: # Try to convert the matched string into a date object to have all the dates with the same format\n",
    "                date = parser.parse(match, fuzzy = False) # Parse convert the match object into a datetime with a strict parsing (fuzzy = False -> takes only set apart dates, not dates in the text) \n",
    "\n",
    "                if 1990 <= date.year <= 2030: # Keep only dates in a reasonable year range\n",
    "                    return date\n",
    "\n",
    "            except:\n",
    "                continue # If parsing fails, ignore this match and move to the next match\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac42d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_date_csv_file(input_csv, output_csv, domain):\n",
    "\n",
    "    df = pd.read_csv(input_csv) # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'html' not in df.columns: # Verify that the html column exists\n",
    "        raise ValueError(f\"The html column is missing in: {input_csv}\")\n",
    "    \n",
    "    df = df[df['url'].str.startswith(domain)] # We want the content of the internal links (at least for mind body green)\n",
    "    \n",
    "    df['date'] = df['html'].apply(extract_date_from_html) # Extract date from the html column\n",
    "\n",
    "    df = df[['url', 'date', 'html']] # Keep columns in the desired order with the 3 columns\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8') # Create a new csv file\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285293c2",
   "metadata": {},
   "source": [
    "Corpus extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15438ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(html):\n",
    "    # Converts an HTML text string for Python\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # Stores the result \n",
    "    paragraphs = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 10]\n",
    "    text = \" \".join(paragraphs)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_csv_file(input_csv, output_csv):\n",
    "\n",
    "    df = pd.read_csv(input_csv) # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'html' not in df.columns: # Verify that the html colon exists\n",
    "        raise ValueError(f\"The html column is missing in: {input_csv}\")\n",
    "\n",
    "    df['corpus_text'] = df['html'].apply(get_corpus) # Cleans the html column\n",
    "\n",
    "    if 'date' in df.columns :\n",
    "        df = df[['url', 'date', 'corpus_text']] # Keep the url and text colon (not the raw html)\n",
    "    else :\n",
    "        df = df[['url', 'corpus_text']]\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8') # Creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24037cd",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2778328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    for tag in soup(['script', 'style', 'noscript']): # Supress any unessecary tags\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=' ', strip=True) # Collect all visible text\n",
    "    text = re.sub(r'\\s+', ' ', text) # Supress any unecessary spaces\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a42db212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_file(input_csv, output_csv):\n",
    "\n",
    "    df = pd.read_csv(input_csv) # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'corpus_text' not in df.columns: # Verify that the html colon exists\n",
    "        raise ValueError(f\"The corpus column is missing in: {input_csv}\")\n",
    "\n",
    "    df['cleaned_text'] = df['corpus_text'].apply(clean_html) # Cleans the html column\n",
    "\n",
    "    if 'date' in df.columns :\n",
    "        df = df[['url', 'date', 'cleaned_text']] # Keep the url and text colon (not the raw html)\n",
    "    else :\n",
    "        df = df[['url', 'cleaned_text']]\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8') # Creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a422b3e",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7e860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_html(text):\n",
    "\n",
    "    text = text.lower() # Convert all characters to lowercase\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text) # Remove reference markers like [1], [23], etc.\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) # Keep only English letters and spaces (remove ponctuation, numbers, and everything that is not a letter)\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "\n",
    "    return text.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29607443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_csv_file(input_csv, output_csv):\n",
    "\n",
    "    df = pd.read_csv(input_csv) # Take the csv file with the cleaned text as the input\n",
    "\n",
    "    if 'cleaned_text' not in df.columns: # Verify that the cleaned text colon exists\n",
    "        raise ValueError(f\"The cleaned text is missing in: {input_csv}\")\n",
    "\n",
    "    df['normalized_text'] = df['cleaned_text'].apply(normalize_html) # Normalize the cleaned text\n",
    "\n",
    "    if 'date' in df.columns :\n",
    "        df = df[['url', 'date', 'normalized_text']] # Keep the url and normalized text \n",
    "    else : \n",
    "        df = df[['url', 'normalized_text']]\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8') # Creates a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15acab",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de249f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_html(text):\n",
    "\n",
    "    stop_words = list(set(stopwords.words('english'))) + [\"'s\"] # Load English stopwords with an \"s\" at their ending\n",
    "    #stem = nltk.stem.SnowballStemmer(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # Transform the text into a list of words\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stopwords\n",
    "    #tokens = [stem.stem(token) for token in tokens] # Apply stemming (racinisation)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Apply lemmatization\n",
    "    #tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    tokens = [t for t in tokens if len(t) > 1] # Remove very short or meaningless tokens \n",
    "    tokens = [t for t in tokens if len(t) < 20] # Remove very long or meaningless tokens \n",
    "    tokens = [t for t in tokens if t not in custom_stopwords] \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c251a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_csv_file(input_csv, output_csv):\n",
    "\n",
    "    df = pd.read_csv(input_csv) # Take the csv file with the normalized text as the input\n",
    "\n",
    "    if 'normalized_text' not in df.columns: # Verify that the normalized text colon exists\n",
    "        raise ValueError(f\"The normalized text is missing in: {input_csv}\")\n",
    "\n",
    "    df['tokenized_text'] = df['normalized_text'].apply(tokenize_html) # Cleans the html colon\n",
    "\n",
    "    if 'date' in df.columns :\n",
    "        df = df[['url', 'date', 'tokenized_text']] # Keep the url and text colon (not the raw html)\n",
    "    else :\n",
    "        df = df[['url', 'tokenized_text']]\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8') # Creates a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d23d03",
   "metadata": {},
   "source": [
    "CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3e335",
   "metadata": {},
   "source": [
    "\"\"\"A CHANGER CAR Y A UN DOMAINE POUR FEEDSPOT ICI JE SUPPOSE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    {\"name\": \"MBG\",\n",
    "     \"domain\": \"https://www.mindbodygreen.com/\"}, # We write the specific function without calling it ()\n",
    "    {\"name\": \"wiki\",\n",
    "     \"domain\": None}, # Wikipedia does not require domain filtering\n",
    "    {\"name\": \"blogs\",\n",
    "     \"domain\": None} \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87f4bd",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08354eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"========================== SOURCES PIPELINE ==========================\"\"\"\n",
    "\n",
    "\"\"\"=== SETUP ===\"\"\"\n",
    "all_tokenized_dicts = {} \n",
    "\n",
    "for source in sources:\n",
    "    source_name = source[\"name\"]\n",
    "    domain = source[\"domain\"]\n",
    "\n",
    "    files = get_filenames(source_name)\n",
    "\n",
    "    print(f\"=== SOURCE PROCESSING : {source_name} ===\")\n",
    "    \n",
    "    print(\"\\n=== DATES EXTRACTION ===\")\n",
    "    \n",
    "    if domain:\n",
    "        print(f\"{BLUE}[{source_name}] Extracting dates and filtering by domain...{RESET}\")\n",
    "        content_date_csv_file(files[\"raw\"], files[\"content_date\"], domain=domain)\n",
    "        print(f\"{GREEN}Content with dates saved to {files['content_date']}{RESET}\")\n",
    "\n",
    "        input_for_corpus = files[\"content_date\"]\n",
    "    else:\n",
    "        input_for_corpus = files[\"raw\"]\n",
    "\n",
    "    print(\"\\n=== CORPUS EXTRACTION ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Extracting the corpus...{RESET}\")\n",
    "    extract_corpus_csv_file(input_for_corpus, files[\"corpus\"])\n",
    "    print(f\"{GREEN}Corpus data saved to {files['corpus']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== CLEANING ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Cleaning the data...{RESET}\")\n",
    "    clean_csv_file(files[\"corpus\"], files[\"cleaned\"])\n",
    "    print(f\"{GREEN}Cleaned text saved to {files['cleaned']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== NORMALIZATION ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Normalizing the data...{RESET}\")\n",
    "    normalize_csv_file(files[\"cleaned\"], files[\"norm\"])\n",
    "    print(f\"{GREEN}Normalized text saved to {files['norm']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== TOKENIZATION ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Tokenizing text...{RESET}\")\n",
    "    df_tokenized = tokenize_csv_file(files[\"norm\"], files[\"tokenized\"])\n",
    "    print(f\"{GREEN}Tokenized data saved in {files['tokenized']}. Rows: {len(df_tokenized)}{RESET}\\n\")\n",
    "\n",
    "    current_dict = dict(zip(df_tokenized['url'], df_tokenized['tokenized_text'])) # We transform the csv file with the tokenized text into a dictionnary to make it feet the function above seen in class\n",
    "    all_tokenized_dicts[source_name] = current_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4168c50",
   "metadata": {},
   "source": [
    "## Term document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d20f9",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_term_document_matrix(documents, min_doc_freq=2, max_doc_freq=None):\n",
    "\n",
    "    # Step 1: Build a vocabulary (unique terms across all documents)\n",
    "    vocabulary = set(token for tokens in documents.values() for token in tokens)  # The vocabulary contains all tokens appearing in the corpus. Each token is listed only once, even if it appears multiple times.\n",
    "\n",
    "    # Step 2: Count term frequencies for each document\n",
    "    term_frequencies = {doc: Counter(tokens) for doc, tokens in documents.items()}  # The dictionnary has the name of a document as a key and a counter of token frequency as value\n",
    "\n",
    "    # Step 3: Construct the term-document matrix (a list of dictionnaries that is transform into pandas dataframe where each column corresponds to a token, each row to a document. Each dictionnary has a token as key and a list of the frequency of its number of appears in all documents as value)\n",
    "    td_matrix = pd.DataFrame(\n",
    "        {term: [term_frequencies[doc].get(term, 0) for doc in documents] for term in vocabulary},\n",
    "        index = documents.keys())  \n",
    "\n",
    "    # Step 4: Filter terms that appear in fewer / more than x documents\n",
    "    document_frequency = (td_matrix > 0).sum(axis=0) # Document frequency counts in how many documents each token appears\n",
    "\n",
    "    if min_doc_freq is not None:\n",
    "        td_matrix = td_matrix.loc[:, document_frequency >= min_doc_freq] # We only keep tokens if they appears a minimum of times. This removes very rare tokens that may not be informative.\n",
    "\n",
    "    if max_doc_freq is not None:\n",
    "        document_frequency = (td_matrix > 0).sum(axis=0)\n",
    "        td_matrix = td_matrix.loc[:, document_frequency <= max_doc_freq] # We only keep tokens if they appears less than a maximum of times. This removes very common tokens that may be uninformative.\n",
    "\n",
    "    vocabulary = td_matrix.columns.tolist() # Update the vocabulary to include only the tokens kept after filtering.\n",
    "\n",
    "    return td_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e367ba",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== BUILDING TERM-DOCUMENT MATRICES ===\")\n",
    "\n",
    "if \"MBG\" in all_tokenized_dicts:\n",
    "    print(f\"{BLUE}[MBG] Building Matrix...{RESET}\")\n",
    "    MBG_td_matrix, MBG_vocab = build_term_document_matrix(all_tokenized_dicts[\"MBG\"], min_doc_freq=4, max_doc_freq=400)\n",
    "    print(\"MBG Matrix shape:\", MBG_td_matrix.shape)\n",
    "    print(MBG_vocab[:20])\n",
    "    print(MBG_td_matrix)\n",
    "\n",
    "if \"wiki\" in all_tokenized_dicts:\n",
    "    print(f\"\\n{BLUE}[Wiki] Building Matrix...{RESET}\")\n",
    "    wiki_td_matrix, wiki_vocab = build_term_document_matrix(all_tokenized_dicts[\"wiki\"], min_doc_freq=2)\n",
    "    print(\"Wiki Matrix shape:\", wiki_td_matrix.shape)\n",
    "    print(wiki_vocab[:20])\n",
    "    print(wiki_td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== MERGING DATASETS ===\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for source in sources:\n",
    "    f_token = get_filenames(source[\"name\"])[\"tokenized\"]\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(f_token)\n",
    "        df['source_origin'] = source[\"name\"] # On garde une trace de l'origine !\n",
    "        all_dfs.append(df)\n",
    "    except:\n",
    "        print(f\"Warning: {f_token} missing\")\n",
    "\n",
    "df_global = pd.concat(all_dfs, ignore_index=True)\n",
    "df_global.to_csv(\"GLOBAL_dataset_tokenized.csv\", index=False)\n",
    "print(f\"{GREEN}SUCCESS ! Final data in GLOBAL_dataset_tokenized.csv{RESET}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e44a64",
   "metadata": {},
   "source": [
    "üåü 2) R√®gle g√©n√©rale (recommand√©e en NLP)\n",
    "Pour un corpus de taille N documents :\n",
    "\n",
    "‚úîÔ∏è min_doc_freq ‚âà 0.5% √† 1% de N\n",
    "‚úîÔ∏è max_doc_freq ‚âà 40% √† 60% de N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c52a75",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73b60f",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d1c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(td_matrix) : # Calculate the TF-IDF matrix from the Term-Document matrix\n",
    "\n",
    "    row_sums = td_matrix.sum(axis=1) # Compute the total number of tokens in each document (= sum on the td_matrix's rows)\n",
    "    tf = td_matrix.div(row_sums, axis=0) # Compute Term Frequency (TF). Each term count is divided by the total number of tokens in the corresponding document (normalization). This accounts for differences in document length.\n",
    "    df = (td_matrix > 0).sum(axis=0) # Compute Document Frequency (DF). For each term (column of the td_matrix), count the number of documents in which the term appears at least once.\n",
    "    N = td_matrix.shape[0] # Compute the total number of documents. This corresponds to the number of rows in the td_matrix.\n",
    "    idf = np.log((N) / (df)) # Compute the IDF (Specificity). Terms that appear in many documents receive a lower IDF, while rare terms receive a higher IDF, making them more informative.\n",
    "    tf_idf = tf.mul(idf, axis=1) # Multiply each item in the tf column by the IDF corresponding so that each term's TF is weighted by its IDF score.\n",
    "\n",
    "    return(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608dcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help us visualize, we transform the urls into readable labels\n",
    "def url_to_label(url):\n",
    "\n",
    "    last_part = url.rstrip('/').split('/')[-1] # We only keep the last part of the url\n",
    "    label = ' '.join([word.capitalize() for word in last_part.replace('-', ' ').replace('_', ' ').split()]) # We delete unwanted element (\"_\") and replace them by a space. The following letter is set as a capital letter.\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266fb00",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca445da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== VISUALIZATION MBG ===\")\n",
    "\n",
    "\"\"\" FOR MINDBODYGREEN \"\"\"\n",
    "\n",
    "# 1. Calcul TF-IDF\n",
    "MBG_tf_idf = tf_idf(MBG_td_matrix)\n",
    "print(MBG_tf_idf)\n",
    "\n",
    "# 2. Mandatory cleaning (Infinity -> NaN -> 0)\n",
    "MBG_tf_idf_clean = MBG_tf_idf.replace([np.inf, -np.inf], np.nan).fillna(0)    \n",
    "\n",
    "# 3. PCA (R√©duction √† 2 dimensions)\n",
    "pca = PCA(n_components=2)\n",
    "MBG_tf_idf_2d = pca.fit_transform(MBG_tf_idf_clean)\n",
    "\n",
    "# 4. DataFrame pour faciliter le plot\n",
    "MBG_tf_idf_2d_df = pd.DataFrame(MBG_tf_idf_2d, columns=['x', 'y'], index=MBG_tf_idf_clean.index)\n",
    "MBG_tf_idf_2d_df['label'] = MBG_tf_idf_2d_df.index.map(url_to_label)\n",
    "\n",
    "# 5. Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(MBG_tf_idf_2d_df['x'], MBG_tf_idf_2d_df['y'], color='blue')\n",
    "\n",
    "# Ajout des labels\n",
    "for _, row in MBG_tf_idf_2d_df.iterrows():\n",
    "    plt.text(row['x']+0.003, row['y']+0.003, row['label'], fontsize=7)\n",
    "\n",
    "plt.title(\"TF-IDF Visualisation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarde au lieu d'afficher (plus s√ªr pour un script)\n",
    "output_img = \"MBG_pca_visualization.png\"\n",
    "plt.savefig(output_img)\n",
    "print(f\"{GREEN}Graph saved to {output_img}{RESET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814f6f3",
   "metadata": {},
   "source": [
    "y'avait pas la suite pour wiki j'ai pas os√© ajouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== VISUALIZATION WIKI ===\")\n",
    "\n",
    "\"\"\" FOR WIKIPEDIA \"\"\"\n",
    "\n",
    "# 1. Calcul TF-IDF\n",
    "wiki_tf_idf = tf_idf(wiki_td_matrix)\n",
    "print(wiki_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36064fa",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee5c19",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c64b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(tf_idf):\n",
    "    #tf_idf_clean = tf_idf.dropna(axis=0, how='any') # Remove rows with NaN values \n",
    "    #tf_idf_clean = tf_idf_clean[tf_idf_clean.sum(axis=1) != 0] # Remove rows that are entirely zero (empty vectors)\n",
    "\n",
    "    tf_idf_clean = tf_idf.replace([np.inf, -np.inf], np.nan).fillna(0) # Replace \"NaN\" with a 0\n",
    "\n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix_tfidf = cosine_similarity(tf_idf_clean) # Each value ranges from 0 (no similarity) to 1 (identical documents).\n",
    "\n",
    "    # Convert to a DataFrame for better readability (optional)\n",
    "    similarity_df_tfidf = pd.DataFrame(similarity_matrix_tfidf, index=tf_idf_clean.index, columns=tf_idf_clean.index) # Rows and columns are labeled with document identifiers (index)\n",
    "\n",
    "    # Display the similarity DataFrame\n",
    "    display(similarity_df_tfidf)\n",
    "    return(similarity_df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(similarity_df):\n",
    "    # Plot the similarity matrix\n",
    "    plt.figure(figsize=(14, 11))\n",
    "    plt.imshow(similarity_df, interpolation='nearest', cmap='viridis')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.title('Document Similarity Matrix')\n",
    "\n",
    "    # Labels = [url_to_label(url) for url in similarity_df.columns] # We labelized the document not with their full link but their label found with the function (url_to_label)\n",
    "    \n",
    "    ax = plt.gca()  # We set the axis to have them set at the right place\n",
    "    # ax.set_xticks(range(len(labels)))\n",
    "    # ax.set_yticks(range(len(labels)))\n",
    "    # ax.set_xticklabels( labels, rotation=45, ha='right', rotation_mode='anchor', fontsize=7)\n",
    "    # ax.set_yticklabels( labels, fontsize=7)\n",
    "\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    ax.set_xlabel('Documents')\n",
    "    ax.set_ylabel('Documents')\n",
    "\n",
    "    # Optionally, annotate the cells with similarity values\n",
    "    \"\"\"for i in range(len(similarity_df)):\n",
    "        for j in range(len(similarity_df)):\n",
    "            ax.text(j, i, f\"{similarity_df.iloc[i, j]:.3f}\", ha='center', va='center', color='white')\"\"\"\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc14980",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a234e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBG_similarity_matrix = similarity_matrix(MBG_tf_idf)\n",
    "print(MBG_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity_matrix(MBG_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be26564",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_similarity_matrix = similarity_matrix(wiki_tf_idf)\n",
    "print(wiki_similarity_matrix)\n",
    "plot_similarity_matrix(wiki_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24982f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sans les modifs de chat (= le code du prof) mais alors les axes d√©conne un peu\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix_tfidf = cosine_similarity(tf_idf) # Each value ranges from 0 (no similarity) to 1 (identical documents).\n",
    "\n",
    "# Convert to a DataFrame for better readability (optional)\n",
    "similarity_df_tfidf = pd.DataFrame(similarity_matrix_tfidf, index=tf_idf.index, columns=tf_idf.index) # Rows and columns are labeled with document identifiers (index)\n",
    "\n",
    "# Display the similarity DataFrame\n",
    "display(similarity_df_tfidf)\n",
    "\n",
    "def plot_similarity_matrix(similarity_df):\n",
    "    # Plot the similarity matrix\n",
    "    plt.figure(figsize=(14, 11))\n",
    "    plt.imshow(similarity_df, interpolation='nearest', cmap='viridis')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.title('Document Similarity Matrix')\n",
    "\n",
    "    labels = [url_to_label(url) for url in similarity_df.columns] # We labelized the document not with their full link but their label found with the function (url_to_label)\n",
    "    \n",
    "    plt.xticks(ticks=range(len(similarity_df.columns)), labels = labels, rotation=45, fontsize=7)  \n",
    "    plt.yticks(ticks=range(len(similarity_df.index)), labels = labels, fontsize=7)\n",
    "    plt.xlabel('Documents')\n",
    "    plt.ylabel('Documents')\n",
    "\n",
    "    # Optionally, annotate the cells with similarity values\n",
    "    for i in range(len(similarity_df)):\n",
    "        for j in range(len(similarity_df)):\n",
    "            plt.text(j, i, f\"{similarity_df.iloc[i, j]:.3f}\", ha='center', va='center', color='white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_similarity_matrix(similarity_df_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c964f2",
   "metadata": {},
   "source": [
    "## Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [tok for tokens in tokenized_MBG_csv[\"tokenized_text\"] for tok in tokens] # Create a list with all the tokens\n",
    "MBG_word_freq = Counter(all_tokens) # Count the number of times a token appears in the list\n",
    "MBG_word_freq.most_common(20) # Print only the 20 most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd200656",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [tok for tokens in tokenized_wiki_csv[\"tokenized_text\"] for tok in tokens] # Create a list with all the tokens\n",
    "wiki_word_freq = Counter(all_tokens) # Count the number of times a token appears in the list\n",
    "wiki_word_freq.most_common(20) # Print only the 20 most frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd3130",
   "metadata": {},
   "source": [
    "## Words cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud( width=900, height=450, background_color=\"white\").generate_from_frequencies(MBG_word_freq)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud( width=900, height=450, background_color=\"white\").generate_from_frequencies(wiki_word_freq)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb783d",
   "metadata": {},
   "source": [
    "## Analyse th√©matique MBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBG_tf_idf_clean = MBG_tf_idf.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\"\"\"Pourquoi c‚Äôest bien ?\n",
    "K‚Äëmeans et PCA refusent les NaN ‚Üí tu √©vites les crashs\n",
    "\n",
    "Tu garantis un espace vectoriel propre\n",
    "\n",
    "Tu √©vites les erreurs silencieuses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow(tf_idf, max_k=10):\n",
    "    inertias = []\n",
    "    n_docs = tf_idf.shape[0]\n",
    "    max_k = min(max_k, n_docs - 1)\n",
    "    K_values = range(2, max_k)\n",
    "\n",
    "    for k in K_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(tf_idf)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(K_values, inertias, marker='o')\n",
    "    plt.xlabel(\"Nombre de clusters (k)\")\n",
    "    plt.ylabel(\"Inertia (distortion)\")\n",
    "    plt.title(\"M√©thode du coude (Elbow Method)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # approx du \"knee\" : le point o√π la diminution ralentit le plus\n",
    "    # on prend le point avec la plus grande \"diff√©rence d'inertie\" successive\n",
    "    deltas = np.diff(inertias)\n",
    "    best_k_idx = np.argmin(deltas) + 1 # +1 car diff d√©cale de 1\n",
    "    best_k = K_values[best_k_idx]\n",
    "    best_inertia = inertias[best_k_idx]\n",
    "\n",
    "    return best_k, best_inertia, K_values, inertias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde43fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k, best_inertia, K_values, inertias = plot_elbow(MBG_tf_idf_clean, max_k=10)\n",
    "print(\"K id√©al :\", best_k)\n",
    "print(\"Inertia :\", best_inertia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92633acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(tf_idf, max_k=10):\n",
    "    scores = []\n",
    "    n_docs = tf_idf.shape[0]\n",
    "    max_k = min(max_k, n_docs - 1)\n",
    "    K_values = range(2, max_k)\n",
    "\n",
    "    for k in K_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(tf_idf)\n",
    "        score = silhouette_score(tf_idf, labels)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(K_values, scores, marker='o')\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Silhouette score\")\n",
    "    plt.title(\"M√©thode du Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    best_score = max(scores) \n",
    "    best_k = K_values[scores.index(best_score)] \n",
    "    return best_k, best_score, list(K_values), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k, best_score, ks, scores = plot_silhouette(MBG_tf_idf_clean) \n",
    "print(\"Best k :\", best_k) \n",
    "print(\"Associated silhouette score :\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8031cf7",
   "metadata": {},
   "source": [
    "Attention : on prend 5 pour k et pas 4 parce que les clusters sont mesur√©s sur base de tokens (= pas de stopwords ou custom_stopwords) sauf que Bert est sur normalized_text et m√™me si on prend dans vectorizer les stopwords, Bert ne les exclut pas donc on prend k+1 car un cluster sera pour les noms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f7d96f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBG_k = 5\n",
    "MBG_kmeans = KMeans(n_clusters=MBG_k, random_state=42)\n",
    "MBG_tf_idf_clean['cluster'] = MBG_kmeans.fit_predict(MBG_tf_idf_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(MBG_tf_idf_clean.drop(columns=['cluster']))\n",
    "\n",
    "MBG_pca_df = pd.DataFrame(coords, columns=['x','y'], index=MBG_tf_idf_clean.index)\n",
    "\n",
    "plt.figure(figsize=(8,6)) \n",
    "plt.scatter(MBG_pca_df['x'], MBG_pca_df['y'], c=MBG_tf_idf_clean['cluster'], cmap='tab10') \n",
    "plt.title(\"Clusters K-means\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e0626",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertopic(normalized_corpus, vectorizer_model, cluster_model, min_words=100, embedding_model_name=\"all-MiniLM-L6-v2\", nr_words=100):\n",
    "    \"\"\"\n",
    "    Pipeline BERTopic avec KMeans (sans HDBSCAN), en for√ßant BERTopic\n",
    "    √† retourner plus que 10 mots par topic gr√¢ce √† KeyBERTInspired.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load corpus\n",
    "    df = pd.read_csv(normalized_corpus)\n",
    "    documents = df[\"normalized_text\"].astype(str).tolist() # Make sure that every document is a string\n",
    "    documents = [d for d in documents if len(d.split()) > min_words] # remove documents with less than a x words (they are too short and only makes noise)\n",
    "\n",
    "    # Embedding model\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # Representation model forcing >10 words\n",
    "    representation_model = KeyBERTInspired(top_n_words=nr_words)\n",
    "\n",
    "    # BERTopic model\n",
    "    topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model, hdbscan_model=cluster_model, representation_model=representation_model, verbose=True)\n",
    "\n",
    "    # Fit and transform\n",
    "    topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "    return topic_model, topics, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f51db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stop_words = list(set(stop_words).union(custom_stopwords))\n",
    "\n",
    "vectorizer_model = CountVectorizer( stop_words=full_stop_words, min_df=4, max_features=5000)\n",
    "\n",
    "MBG_bert, MBG_topics, MBG_topics_probs = bertopic( \"data/normalized_MBG_corpus.csv\", vectorizer_model=vectorizer_model, cluster_model=MBG_kmeans, min_words=100, embedding_model_name=\"all-MiniLM-L6-v2\", nr_words=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_topics(topic_model, top_words=3, max_words=100, min_score=0.1):\n",
    "    \"\"\"\n",
    "    R√©sume les topics d'un mod√®le BERTopic :\n",
    "    - Cr√©e un nom simple pour chaque topic √† partir des mots les plus importants\n",
    "    - Note le nombre de documents par th√®me\n",
    "    - Identifie le th√®me principal (topic avec le plus de documents)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_model : BERTopic\n",
    "        Mod√®le BERTopic d√©j√† entra√Æn√©.\n",
    "    top_words : int\n",
    "        Nombre de mots √† utiliser pour cr√©er le nom du th√®me.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    themes : dict\n",
    "        Dictionnaire {nom_du_th√®me : {\"words\": [...], \"count\": N_docs}}.\n",
    "    main_theme : dict\n",
    "        Infos du th√®me principal : {'topic_id', 'name', 'words', 'count'}.\n",
    "    topic_df : pd.DataFrame\n",
    "        DataFrame avec Topic ID, Count, Name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1Ô∏è‚É£ Infos des topics\n",
    "    topic_info = topic_model.get_topic_info() # Topic, Count, Name\n",
    "    topic_info = topic_info[topic_info.Topic != -1] # ignorer les outliers\n",
    "\n",
    "    main_topic_id = (\n",
    "        topic_info.sort_values(by=\"Count\", ascending=False)\n",
    "        .Topic\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    # 2Ô∏è‚É£ Cr√©er le dictionnaire themes avec le nombre de documents\n",
    "    themes = {}\n",
    "    for topic_id in topic_info.Topic:\n",
    "        words_scores = topic_model.get_topic(topic_id)\n",
    "        # üî• filtrage par score + limite\n",
    "        words = [\n",
    "            word\n",
    "            for word, score in words_scores\n",
    "            if score >= min_score\n",
    "        ][:max_words]\n",
    "        # words = [w for w, _ in topic_model.get_topic(topic_id)]\n",
    "        theme_name = \", \".join(words[:top_words])\n",
    "        count_docs = topic_info.loc[topic_info.Topic == topic_id, \"Count\"].values[0]\n",
    "        is_main_topic = (topic_id == main_topic_id)\n",
    "        themes[theme_name] = {\"keywords\": words} \n",
    "        \n",
    "        print(f\"Theme: {theme_name}\")                            \n",
    "        print(\"Numbers of documents\", count_docs)\n",
    "        print(words)\n",
    "        \n",
    "        if is_main_topic:\n",
    "            main_topic_words = words\n",
    "            print(\"-> This is the main topic\")\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    \n",
    "    # 4Ô∏è‚É£ Retourner un DataFrame complet\n",
    "    topic_df = topic_info.copy()\n",
    "    \n",
    "    return themes, main_topic_words, topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "themes, main_topic_words, topic_df = summarize_topics(MBG_bert, top_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "11c4e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_topic_words(main_topic_words):\n",
    "    filtered_words = []\n",
    "    for word in main_topic_words :\n",
    "        if len(word) <= 2 :\n",
    "            continue\n",
    "        if word in full_stop_words :\n",
    "            continue\n",
    "        filtered_words.append(word)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c26435",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_topic_words_filtered = filter_topic_words(main_topic_words)\n",
    "print(len(main_topic_words))\n",
    "print(len(main_topic_words_filtered))\n",
    "print(main_topic_words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame\n",
    "df = pd.DataFrame({\"main_topic_words_filtered\": [main_topic_words_filtered]}) # note les [ ] pour que ce soit UNE cellule\n",
    "\n",
    "# Sauvegarder\n",
    "df.to_csv(\"data/main_topic_words_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328975df",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = MBG_k # Define the number of topics you want the LDA model to extract from the documents.\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42) # Create an LDA (Latent Dirichlet Allocation) model instance. random_state = seed for reproducibility (ensures the same results each time)\n",
    "\n",
    "lda_model.fit(MBG_td_matrix) # LDA learns the topic distributions for each document and the word distributions for each topic. We use term-document matrix as the vector for the LDA matrix (!! not tf-idf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER (??)\n",
    "\n",
    "MBG_words = MBG_td_matrix.columns.tolist()\n",
    "MBG_themes = {}\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    distinctiveness = topic / lda_model.components_.sum(axis=0)\n",
    "    top_idx = distinctiveness.argsort()[::-1][:3]\n",
    "    topic_name = \", \".join(MBG_words[i] for i in top_idx)\n",
    "\n",
    "    MBG_themes[topic_name] = [MBG_words[i] for i in topic.argsort()[::-1][:10]]\n",
    "\n",
    "MBG_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = MBG_td_matrix.columns.tolist() # Get the list of words (terms) corresponding to the columns of the td_matrix.\n",
    "themes = {} # Initialize an empty dictionary to store topics. Key = topic name (most frequent word), Value = set of top words for that topic.\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_): # For each topic in the LDA model. lda_model.components_ is a matrix of shape (n_topics, n_words). Each row corresponds to a topic and contains the importance of each word for that topic.\n",
    "    \n",
    "    top_words_idx = topic.argsort()[::-1][:10] # We keep only the 10 main words that have the most relevance (topic.argsort() sorts word indices in ascending order by their importance)\n",
    "    top_words = [words[i] for i in top_words_idx] # Convert the indices into actual word strings using the 'words' list.\n",
    "\n",
    "    topic_name = top_words[0] # We give a \"name\" to the topic (which is the most frequant word of the list)\n",
    "\n",
    "    \"\"\"topic_word_scores = lda_model.components_[topic_idx]\n",
    "    distinctiveness = topic_word_scores / lda_model.components_.sum(axis=0)\n",
    "    top_idx = distinctiveness.argsort()[::-1][:1]\n",
    "    topic_name = \", \".join(words[i] for i in top_idx)\"\"\"\n",
    "\n",
    "    themes[topic_name] = set(top_words) # Store the topic in the dictionary.\n",
    "\n",
    "print(themes) \n",
    "\n",
    "for name, words_set in themes.items():\n",
    "    print(f\"{name}: {words_set}\") # For a clearer view of the topics and their corresponding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Word cloud for every topic\n",
    "\n",
    "words = MBG_td_matrix.columns.tolist()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    # Get the top words and their scores\n",
    "    top_words_idx = topic.argsort()[::-1][:20] # top 20 words\n",
    "    top_words = [words[i] for i in top_words_idx]\n",
    "    word_freq = {words[i]: topic[i] for i in top_words_idx} # word -> importance\n",
    "\n",
    "    topic_name = top_words[0]\n",
    "    print(topic_name)\n",
    "\n",
    "    # Create the WordCloud\n",
    "    wc = WordCloud( width=900, height=450, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "\n",
    "    # Display the WordCloud\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(topic_name, fontsize=16, color='black') # Topic name at the top\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b6be5",
   "metadata": {},
   "source": [
    "Trouver les th√®mes les pluc r√©currents et les noms qu'on devrait leur donner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = lda_model.transform(MBG_td_matrix)\n",
    "theme_strength = doc_topic_matrix.sum(axis=0)\n",
    "theme_order = theme_strength.argsort()[::-1]\n",
    "print(theme_order)\n",
    "print(theme_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c429a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Nommer le th√®me avec le mot le plus ‚Äúdistinctif‚Äù\n",
    "Au lieu de prendre le mot le plus fr√©quent dans le th√®me, tu prends le mot qui :\n",
    "\n",
    "est tr√®s important dans ce th√®me\n",
    "mais peu important dans les autres th√®mes\n",
    "C‚Äôest ce qu‚Äôon appelle un mot discriminant. \"\"\"\n",
    "\n",
    "topic_word_scores = lda_model.components_[topic_idx]\n",
    "distinctiveness = topic_word_scores / lda_model.components_.sum(axis=0)\n",
    "top_idx = distinctiveness.argsort()[::-1][:4]\n",
    "topic_name = \", \".join(words[i] for i in top_idx)\n",
    "print(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad56de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nommer le th√®me avec les documents qui lui appartiennent\n",
    "# Tu regardes les documents o√π ce th√®me est dominant, puis tu en extrais les mots les plus fr√©quents\n",
    "\n",
    "doc_topic = lda_model.transform(MBG_td_matrix)\n",
    "dominant_docs = MBG_td_matrix.iloc[doc_topic[:, topic_idx].argsort()[::-1][:5]]\n",
    "top_words = dominant_docs.sum().sort_values(ascending=False).head(4).index\n",
    "topic_name = \", \".join(top_words)\n",
    "print(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSE CROIS√âE K‚ÄëMEANS ‚Üî LDA\n",
    "\n",
    "doc_topic = lda_model.transform(MBG_td_matrix)\n",
    "\"\"\"LDA renvoie une matrice document ‚Üí th√®mes.\n",
    "Chaque ligne = un document\n",
    "Chaque colonne = un th√®me\n",
    "Chaque valeur = probabilit√© que le document appartienne √† ce th√®me\"\"\"\n",
    "\n",
    "lda_dominant = doc_topic.argmax(axis=1) \n",
    "\"\"\"Pour chaque document, on prend le th√®me dominant (celui avec la probabilit√© la plus haute).\"\"\"\n",
    "\n",
    "pd.crosstab(MBG_tf_idf_clean['cluster'], lda_dominant)\n",
    "\"\"\"On compare :\n",
    "les clusters K-means (g√©om√©triques)\n",
    "les th√®mes LDA (s√©mantiques)\n",
    "C‚Äôest une matrice de correspondance.\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "col_0\t0\t1\t2\t3\n",
    "cluster\t\t\t\t\n",
    "0\t0\t1\t0\t0\n",
    "1\t1\t1\t1\t0\n",
    "2\t4\t1\t0\t1\n",
    "\n",
    "‚úîÔ∏è K-means et LDA ne racontent pas la m√™me histoire\n",
    "K-means regroupe selon la g√©om√©trie TF-IDF\n",
    "\n",
    "LDA regroupe selon la distribution des mots\n",
    "\n",
    "‚úîÔ∏è Ton tableau montre :\n",
    "un cluster tr√®s coh√©rent (cluster 0)\n",
    "\n",
    "un cluster tr√®s incoh√©rent (cluster 1)\n",
    "\n",
    "un cluster partiellement coh√©rent (cluster 2)\n",
    "\n",
    "üëâ Ton clustering n‚Äôest pas tr√®s align√© avec les th√®mes LDA.\n",
    "\n",
    "Ce n‚Äôest pas grave, mais √ßa montre que K-means n‚Äôest pas le meilleur outil pour d√©tecter des th√®mes dans un petit corpus.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = list(themes.keys())\n",
    "\n",
    "doc_topic = lda_model.transform(MBG_td_matrix)\n",
    "lda_dominant = doc_topic.argmax(axis=1) \n",
    "cross = pd.crosstab(MBG_tf_idf_clean['cluster'], lda_dominant)\n",
    "\n",
    "cross.columns = [topic_names[i] for i in cross.columns]\n",
    "cross.index.name = \"Clusters K-means\" \n",
    "cross.columns.name = \"Th√®mes LDA\"\n",
    "\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison automatique K‚Äëmeans ‚Üî LDA\n",
    "\n",
    "def compare_kmeans_lda(tfidf_df, lda_model, td_matrix):\n",
    "    \"\"\"\n",
    "    Compare les clusters K-means et les th√®mes LDA.\n",
    "    Retourne :\n",
    "    - une matrice crois√©e\n",
    "    - une interpr√©tation automatique\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Th√®me dominant par document (LDA)\n",
    "    doc_topic = lda_model.transform(td_matrix)\n",
    "    lda_dominant = doc_topic.argmax(axis=1)\n",
    "\n",
    "    # 2. Cluster K-means d√©j√† dans tfidf_df['cluster']\n",
    "    clusters = tfidf_df['cluster']\n",
    "\n",
    "    # 3. Matrice crois√©e\n",
    "    cross = pd.crosstab(clusters, lda_dominant)\n",
    "\n",
    "    print(\"=== Matrice crois√©e K-means ‚Üî LDA ===\")\n",
    "    print(cross)\n",
    "    print()\n",
    "\n",
    "    # 4. Interpr√©tation automatique\n",
    "    print(\"=== Interpr√©tation automatique ===\")\n",
    "    for cluster_id in cross.index:\n",
    "        row = cross.loc[cluster_id]\n",
    "        dominant_topic = row.idxmax()\n",
    "        count = row.max()\n",
    "\n",
    "        print(f\"- Cluster {cluster_id} ‚Üí Th√®me LDA dominant : {dominant_topic} ({count} documents)\")\n",
    "\n",
    "    return cross\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf080d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_kmeans_lda(MBG_tf_idf_clean, lda_model, MBG_td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d978e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theme_from_url(url, td_matrix, lda_model, topic_names):\n",
    "    \"\"\"\n",
    "    Retourne le th√®me dominant associ√© √† une URL d√©j√† pr√©sente dans la td_matrix.\n",
    "    \n",
    "    url : str\n",
    "        L'URL du document (cl√© de la td_matrix)\n",
    "    td_matrix : DataFrame\n",
    "        La matrice terme-document utilis√©e pour entra√Æner LDA\n",
    "    lda_model : LatentDirichletAllocation\n",
    "        Le mod√®le LDA d√©j√† entra√Æn√©\n",
    "    topic_names : list\n",
    "        Liste ordonn√©e des noms de th√®mes (ex: [\"cortisol\", \"vo2\", ...])\n",
    "    \"\"\"\n",
    "\n",
    "    # V√©rifier que l'URL existe dans la matrice\n",
    "    if url not in td_matrix.index:\n",
    "        raise ValueError(f\"L'URL '{url}' n'existe pas dans la td_matrix.\")\n",
    "\n",
    "    # Extraire la ligne correspondant au document\n",
    "    doc_vector = td_matrix.loc[url].values.reshape(1, -1)\n",
    "\n",
    "    # Appliquer LDA sur ce document\n",
    "    topic_distribution = lda_model.transform(doc_vector)[0]\n",
    "\n",
    "    # Trouver le th√®me dominant\n",
    "    dominant_topic_id = topic_distribution.argmax()\n",
    "\n",
    "    # Retourner le nom du th√®me\n",
    "    return topic_names[dominant_topic_id], topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_theme_from_url(\"https://www.mindbodygreen.com/articles/journey-to-long-covid-diagnosis-and-how-starting-to-heal\", MBG_td_matrix, lda_model, topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_theme(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv) # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'id' not in df.columns: # Verify that the html colon exists\n",
    "        raise ValueError(f\"The id column is missing in: {input_csv}\")\n",
    "\n",
    "    df['id'] = df['theme'].apply(get_theme_from_url) # Cleans the html column\n",
    "    df = df[['id', 'label', 'theme']] # Keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8') # Creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fabfc",
   "metadata": {},
   "source": [
    "## Analyse temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_themes(tokens, themes): # Count the number of times a theme (or associated word) appears in each document (/ doc's tokens)\n",
    "    counts = Counter()\n",
    "    token_set = set(tokens)\n",
    "    for theme, keywords in themes.items():\n",
    "        counts[theme] = len(token_set & keywords)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed852941",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_MBG = tokenized_MBG_csv.copy() # To be safer, we creat a copy of the dataframe\n",
    "timeline_MBG ['date'] = pd.to_datetime(timeline_MBG['date'], errors='coerce') # Transforms the dates into correct dates frame and if the data is not convertible it becomes \"Not a Time\"\n",
    "timeline_MBG = timeline_MBG.dropna(subset=['date']) # Delete all rows whith unvalid dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040afa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for i, row in timeline_MBG.iterrows(): # For every line in the dataframe (timeline_MBG)\n",
    "    tokens = row['tokenized_text'] # Collect the tokens in the correct column\n",
    "\n",
    "    theme_counts = count_themes(tokens, themes) # Count the number of times the theme appears in the article's tokens\n",
    "\n",
    "    for theme, count in theme_counts.items(): # For every theme and number of times it appears\n",
    "        rows.append({ \"date\": row['date'],  \"theme\": theme, \"count\": count }) # Construction of a dictionnary for each date and for each theme and the number of times it appears\n",
    "\n",
    "theme_timeline_MBG = pd.DataFrame(rows) # Transform the list of dictionaaries in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_evolution_timeline_MBG = ( theme_timeline_MBG\n",
    "    .groupby(['date', 'theme'])['count'] # Group the datas (theme_timeline_MBG) according to their date and theme\n",
    "    .sum() # If multiples rows of theme_timeline_MBG has the same group (= same date and theme) it is sum to form only one row\n",
    "    .reset_index() # Important to not have the columns changed into date, theme (which is done automatically by groupby)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba21dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot par jour\n",
    "pivot = ( daily_evolution_timeline_MBG .pivot(index='date', columns='theme', values='count') .fillna(0)) # We pivot the table\n",
    "\n",
    "# Plot\n",
    "pivot.plot(figsize=(12,6))\n",
    "plt.title(\"Daily evolution of themes in lifestyle blogs\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.legend(title=\"Themes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aab68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lissage par moyenne mobile sur 7 jours  -> A supprimer\n",
    "daily_smoothed = pivot.rolling(window=7).mean()\n",
    "\n",
    "daily_smoothed.plot(figsize=(14,6))\n",
    "plt.title(\"√âvolution quotidienne liss√©e (moyenne mobile 7 jours)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Occurrences moyennes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "df = pd.read_csv(\"data/normalized_MBG_corpus.csv\")\n",
    "documents = df[\"normalized_text\"].astype(str).tolist()\n",
    "documents = [d for d in documents if len(d.split()) > 100] # Remove documents with less than a 100 words (they are too short and only makes noise)\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # It transforms every document in a vector which have meaning \n",
    "\n",
    "# Vectorizer\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=3,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# KMeans clustering (NO hdbscan)  -> clustering of documents per topic (we can ask for x cluster)\n",
    "cluster_model = KMeans(\n",
    "    n_clusters=6,\n",
    "    random_state=42,\n",
    "    n_init=\"auto\"\n",
    ")\n",
    "\n",
    "# BERTopic without hdbscan\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    hdbscan_model=cluster_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # We keep 2 dimensions to construct the graph\n",
    "bert_2d = pca.fit_transform(topic_model) # Compute the vectors in 2 D\n",
    "\n",
    "bert_2d_df = pd.DataFrame(tf_idf_2d, columns=['x', 'y'], index=topic_model.index) # Compute a dataframe pandas\n",
    "bert_2d_df['label'] = bert_2d_df.index.map(url_to_label)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(bert_2d_df['x'], bert_2d_df['y'], color='blue')\n",
    "\n",
    "# To delete if we don't want the labels\n",
    "for _, row in bert_2d_df.iterrows():\n",
    "    plt.text(row['x']+0.003, row['y']+0.003, row['label'], fontsize=7)\n",
    "\n",
    "plt.title(\"TF-IDF Visualisation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "k_range = range(2, 16)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    labels = kmeans.fit_predict(topic_model)\n",
    "    \n",
    "    score = silhouette_score(topic_model, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "    print(f\"k = {k} | Silhouette Score = {score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.xlabel(\"Nombre de clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score pour diff√©rents nombres de clusters\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0889675",
   "metadata": {},
   "source": [
    "# Link analysis sur text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f1af631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_nodes_path = r\"C:\\Users\\Violaine\\OneDrive\\Ecole\\Master 1\\Web mining\\Projet Web Mining\\Projet_Web_Mining\\data\\wikipedia_nodes.csv\"\n",
    "wiki_edges_path = r\"C:\\Users\\Violaine\\OneDrive\\Ecole\\Master 1\\Web mining\\Projet Web Mining\\Projet_Web_Mining\\data\\wikipedia_edges.csv\"\n",
    "\n",
    "wiki_nodes = pd.read_csv(wiki_nodes_path)\n",
    "wiki_edges = pd.read_csv(wiki_edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1Ô∏è‚É£ Charger les donn√©es\n",
    "# ---------------------------\n",
    "\n",
    "G = nx.DiGraph()\n",
    "# Ajouter les n≈ìuds avec l'attribut th√®me\n",
    "for idx, row in wiki_nodes.iterrows():\n",
    "    G.add_node(row['id'], theme=row['label']) # A changer par theme !!\n",
    "\n",
    "# Ajouter les liens\n",
    "for idx, row in wiki_edges.iterrows():\n",
    "    G.add_edge(row['source'], row['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter liens intra-th√®me et inter-th√®me  \n",
    "intra = 0\n",
    "inter = 0\n",
    "\n",
    "for u, v in G.edges():\n",
    "    if G.nodes[u]['theme'] == G.nodes[v]['theme']:\n",
    "        intra += 1\n",
    "    else:\n",
    "        inter += 1\n",
    "\n",
    "total_links = G.number_of_edges()\n",
    "print(f\"Liens intra-th√®me : {intra} ({intra/total_links:.2%})\") # -> Homophilie th√©matique\n",
    "print(f\"Liens inter-th√®me : {inter} ({inter/total_links:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G)\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "indegree = dict(G.in_degree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0702a3",
   "metadata": {},
   "source": [
    "## Analyse th√©matique Wikipedia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
