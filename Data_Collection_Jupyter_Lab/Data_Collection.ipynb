{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97dde87c",
   "metadata": {},
   "source": [
    "# **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecac42f",
   "metadata": {},
   "source": [
    "## **Data Collection of MindBodyGreen and Wikipedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a76d6",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, deque\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re \n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import ast  # Useful to convert a string in a list\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "from urllib3.exceptions import LocationParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "540f5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Required for new versions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')   # Required for the lemmatiser\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ac71",
   "metadata": {},
   "source": [
    "## Constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF COLORS ---\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8001b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF HEADER ---\n",
    "\n",
    "HEADER_BROWSER = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\"}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADER_BROWSER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb002d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF FILTERS ---\n",
    "\n",
    "filters_noise = [\n",
    "    # E-commerce\n",
    "    \"/product\", \"/products\", \"/shop\", \"/store\", \"/cart\", \"/checkout\",\n",
    "    # User\n",
    "    \"/account\", \"/login\", \"/register\", \"/profile\", \n",
    "    # Legal and Administrative pages\n",
    "    \"/about\", \"/contact\", \"/privacy\", \"/terms\", \"/policy\", \"/legal\", \"/accessibility\", \n",
    "    \"/editorial-process\", \"/data-collection\", \"/disclaimer\", \"/cookies\", \"/sponsor\",\n",
    "    \"/advertise\", \"/jobs\", \"/faq\", \"/help\", \"/wc\", \n",
    "    # Site Structure\n",
    "    \"/tag\", \"/search\", \"/author\",\n",
    "    \"mailto:\", \"tel:\", \"javascript:\", \".jpg\", \".png\",\n",
    "    \"youtube.com\", \"instagram.com\", \"facebook.com\", \"twitter.com\", \"x.com\",\n",
    "    \"pinterest.com\", \"linkedin.com\", \"tiktok.com\", \"amazon.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe11fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF SEED LINKS FOR WIKIPEDIA ---\n",
    "\n",
    "wiki_seed_pages = [ # Our selection of a few pages = starting list\n",
    "    \"https://en.wikipedia.org/wiki/Lifestyle\",\n",
    "    \"https://en.wikipedia.org/wiki/Lifestyle_trends_and_media\",\n",
    "    \"https://en.wikipedia.org/wiki/Self-care\",\n",
    "    \"https://en.wikipedia.org/wiki/Physical_fitness\",\n",
    "    \"https://en.wikipedia.org/wiki/Healthy_diet\",\n",
    "    \"https://en.wikipedia.org/wiki/Travel\",\n",
    "    \"https://en.wikipedia.org/wiki/Outdoor_recreation\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_living\",\n",
    "    \"https://en.wikipedia.org/wiki/Fashion\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1637a4",
   "metadata": {},
   "source": [
    "## Definitions implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(source_name): \n",
    "    # Create all files from a source using the name specified in the source cell [21]\n",
    "    \n",
    "    directory_name = \"data\"\n",
    "    if not os.path.exists(directory_name): # Checks if the 'data' folder exists\n",
    "        os.makedirs(directory_name) # Setup the output directory\n",
    "        print(f\"{GREEN}Created directory : {directory_name}{RESET}\")\n",
    "    \n",
    "    # We assume that source_name contains no spaces. If changed, add : source_name = source_name.replace(\" \", \"_\")\n",
    "\n",
    "    return {\"raw\":       os.path.join(directory_name, f\"{source_name}_raw_html.csv\"),\n",
    "            \"nodes\" :    os.path.join(directory_name, f\"{source_name}_nodes.csv\"),\n",
    "            \"edges\" :    os.path.join(directory_name, f\"{source_name}_edges.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url) :\n",
    "    # Fetches the content of a URL\n",
    "    \n",
    "    time.sleep(random.uniform(1, 3)) # Random delay between 1 and 3 seconds to avoid overloading the server\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, timeout=10) # Timeout set to 10s to prevent the crawler from hanging on slow pages\n",
    "        \n",
    "        if response.status_code == 200: \n",
    "            return response # If the status code is not OK (200), the function returns none and an error message\n",
    "        else:\n",
    "            print(f\"Failed to fetch the url: {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    except LocationParseError: \n",
    "        return None\n",
    "\n",
    "    except requests.RequestException: # Catch network-related errors (DNS failure, connection refused, etc.)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98a2c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_soup(url): \n",
    "    # Parses the HTML content of a URL into a BeautifulSoup object\n",
    "    \n",
    "    response = fetch_url(url) \n",
    "\n",
    "    if response: # If the fetch was successful, parse the HTML\n",
    "        return BeautifulSoup(response.text, 'html.parser') \n",
    "    else: # If the response is not none, the function return the beautiful soup object\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_breadth_first(url, max_levels=1): \n",
    "    # Crawls a website using Breadth-First Search (BFS)\n",
    "\n",
    "    queue = deque([(url, 0)]) # Initialize BFS queue with (url, current_depth)\n",
    "    visited_links = [url]  # Track visited URLs to avoid duplicates (list preserves order)\n",
    "    last_level_links = [] # Store links found at the maximum level\n",
    "    dico = {} # For the graph structure {parent : [child1, child2]}\n",
    "\n",
    "    while queue:\n",
    "        current_url, current_level = queue.popleft()\n",
    "\n",
    "        if current_level == max_levels: # if the limit is reached, we store the links in the list\n",
    "            if current_url not in last_level_links:\n",
    "                last_level_links.append(current_url)\n",
    "            continue\n",
    "        elif current_level > max_levels:\n",
    "            continue\n",
    "    \n",
    "        soup = to_soup(current_url) # Fetching and parsing\n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        dico.setdefault(current_url, [])\n",
    "\n",
    "        content_area = soup.body # Broad extraction scope: analysis of the entire <body> to retrieve as many links as possible\n",
    "        if not content_area:\n",
    "            continue\n",
    "\n",
    "        links = sorted(content_area.find_all('a', href=True), key=lambda x: x.get('href')) # Extract all 'a' tags with an 'href' attribute and sort alphabetically\n",
    "        \n",
    "        for item in links :\n",
    "            href = item.get('href') # href is short for Hypertext REFerence. It indicates the destination of the link\n",
    "            full_url = urljoin(current_url, href)\n",
    "            full_url = full_url.split('?')[0].split('#')[0] # Cut at the \"?\" of \"#\" and keep only the beginning : [0]\n",
    "            full_url = full_url.lower()\n",
    "\n",
    "            if any(filter in full_url for filter in filters_noise):\n",
    "                continue\n",
    "            \n",
    "            dico[current_url].append(full_url) # Add edge to the adjacency list (Parent -> Child)\n",
    "\n",
    "            if full_url not in visited_links: # If the link is new\n",
    "                visited_links.append(full_url) \n",
    "                queue.append((full_url, current_level + 1)) # Add it to the queue for future crawling\n",
    "                    \n",
    "    return visited_links, last_level_links, dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_depth_first(start_url, visited_links, dico, max_depth=3, max_links_per_page=10, current_depth=0): \n",
    "    #Crawls a website using Depth-First Search (DFS)\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "\n",
    "    soup = to_soup(start_url) # Fetching and parsing\n",
    "    if not soup :\n",
    "        return\n",
    "\n",
    "    dico.setdefault(start_url, []) # Ensure the current URL exists in the adjacency list\n",
    "    links_followed = 0 # Counter to max_links_per_page\n",
    "\n",
    "    content_area = soup.body # Broad extraction scope: analysis of the entire <body> to retrieve as many links as possible\n",
    "    if not content_area:\n",
    "        return\n",
    "    \n",
    "    links = sorted(content_area.find_all('a', href=True), key=lambda x: x.get('href')) # Extract all 'a' tags with an 'href' attribute and sort alphabetically\n",
    "    \n",
    "    for link in links:\n",
    "        if links_followed >= max_links_per_page: # Maximum number of links followed (crawled) from a given page\n",
    "            break\n",
    "\n",
    "        href = link.get('href')\n",
    "        full_url = urljoin(start_url, href) # Convert relative paths to absolute URLs\n",
    "        full_url = full_url.split('?')[0].split('#')[0] # Cut at the \"?\" of \"#\" and keep only the beginning : [0]\n",
    "        full_url = full_url.lower()\n",
    "\n",
    "        if any(filter in full_url for filter in filters_noise):\n",
    "                continue\n",
    "\n",
    "        dico[start_url].append(full_url) # Add edge to the adjacency list (Parent -> Child)\n",
    "\n",
    "        if full_url not in visited_links: # If the link is new, explore it deeply before moving to the next one\n",
    "            visited_links.append(full_url)\n",
    "            links_followed += 1\n",
    "\n",
    "            # Recursive call: dive deeper into this branch\n",
    "            extract_links_depth_first( start_url=full_url, visited_links=visited_links, dico=dico, max_depth=max_depth, max_links_per_page=max_links_per_page, current_depth=current_depth + 1)\n",
    "    \n",
    "    return visited_links, dico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_depth_from_list(max_links_to_dfs, start_links, visited_links, dico, max_depth=3, max_links_per_page=10):\n",
    "    # Function to launch multiple DFS crawls from a list of seed URLs\n",
    "    \n",
    "    if visited_links is None:\n",
    "        visited_links = []\n",
    "    \n",
    "    sorted_links = sorted(start_links) # Sorted because same input order = same results\n",
    "    \n",
    "    for link in sorted_links:\n",
    "        if link not in visited_links:\n",
    "            visited_links.append(link)\n",
    "\n",
    "    for start_url in sorted_links[:max_links_to_dfs]: # Maximum number of seeds (starting points) that we want to crawl in depth  \n",
    "        extract_links_depth_first(start_url=start_url, visited_links=visited_links, \n",
    "                                  dico=dico, max_depth=max_depth, \n",
    "                                  max_links_per_page=max_links_per_page, current_depth=0)\n",
    "\n",
    "    return visited_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_MBG_seed_links(url):\n",
    "    # Definition of our parameters for Feedspots\n",
    "    MAX_LINKS_TO_DFS = 30\n",
    "    MAX_BREADTH_LEVEL = 1\n",
    "    MAX_DEPTH = 2\n",
    "    MAX_LINKS_PER_PAGE = 8 # Number of links per DFS\n",
    "\n",
    "    # Breadth-First Search (wide exploration)\n",
    "    breadth_links, last_level_links, dico = extract_links_breadth_first(url, max_levels=MAX_BREADTH_LEVEL)\n",
    "\n",
    "    # Depth-First Search from the BFS frontier\n",
    "    depth_links = extract_links_depth_from_list(max_links_to_dfs=MAX_LINKS_TO_DFS,\n",
    "                                                start_links=last_level_links, \n",
    "                                                visited_links=breadth_links.copy(), \n",
    "                                                dico=dico, max_depth=MAX_DEPTH, \n",
    "                                                max_links_per_page=MAX_LINKS_PER_PAGE)\n",
    "\n",
    "    all_links = set(breadth_links) | set(depth_links) # Union of BFS and DFS results (order is not preserved, but duplicates are removed)\n",
    "    \n",
    "    return list(all_links), dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4380657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_links(url):\n",
    "    # Extracts internal Wikipedia article links from a given Wikipedia page\n",
    "    \n",
    "    wiki_links = set()\n",
    "\n",
    "    soup = to_soup(url) \n",
    "    if not soup:\n",
    "        return set() # Returns a list of URLs pointing to Wikipedia articles only\n",
    "\n",
    "    # Focus on : Main content paragraphs (article body)\n",
    "    main_content = soup.find_all('div', class_=\"mw-content-ltr mw-parser-output\")\n",
    "    for div in main_content:\n",
    "        for p in div.find_all('p'): # Target: paragraphs (<p>) within the class (main content div)\n",
    "            for item in p.find_all('a', href=True):\n",
    "                href = item.get('href')\n",
    "                if href.startswith(\"/wiki/\") and \":\" not in href: # Keep links \"/wiki/...\" and exclude pages containing \":\"\n",
    "                    wiki_links.add(urljoin(url, href))\n",
    "\n",
    "    # Focus on : \"See also\" section (related articles)\n",
    "    see_also = soup.find_all('div', class_='div-col')\n",
    "    for div in see_also:\n",
    "        for li in div.find_all('li'): # Target: lists (<li>) within columns ('div-col')\n",
    "            for item in li.find_all('a', href=True):\n",
    "                href = item.get('href')\n",
    "                if href.startswith(\"/wiki/\") and \":\" not in href:\n",
    "                    wiki_links.add(urljoin(url, href))\n",
    "\n",
    "    return list(wiki_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd31400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wiki_seed_links(wiki_seed_pages):\n",
    "    \n",
    "    wiki_all_links = set(wiki_seed_pages) # Set() avoids duplication, unlike lists\n",
    "    dico_wiki_links = {}\n",
    "    \n",
    "    for url in wiki_seed_pages:\n",
    "        crawl_links = extract_wikipedia_links(url)\n",
    "        dico_wiki_links[url] = crawl_links\n",
    "        wiki_all_links.update(crawl_links) # Only add new links\n",
    "\n",
    "    return list(wiki_all_links), dico_wiki_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f13838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"{RED}No data to save.{RESET}\")\n",
    "        return\n",
    "\n",
    "    # Warn if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{BLUE}Note: '{filename}' already exists and will be overwritten{RESET}\")\n",
    " \n",
    "    # Get the keys from the first dictionary for the CSV header from get_html_content\n",
    "    try:\n",
    "        fieldnames = data[0].keys() # Detection of the existing colons in the data file\n",
    "    \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f: # Opens the csv file as utf-8 (= character encoding system)\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL) # Initialising a writer to write the dictionary into the csv file\n",
    "            writer.writeheader() # Writes the colons headers\n",
    "            writer.writerows(data) # Writes the rows\n",
    "\n",
    "        print(f\"{GREEN}Success! Data saved to: {RESET}{os.path.abspath(filename)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{RED}Error saving file: {e}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(links, output_csv, max=None):\n",
    "    # Fetches raw HTML content for a list of URLs\n",
    "\n",
    "    content = []\n",
    "    failed_links = []\n",
    "\n",
    "    for link in links[:max]:\n",
    "        response = fetch_url(link)\n",
    "        \n",
    "        if response:\n",
    "            content.append({'url': link, 'html': response.text}) # Store raw HTML as text for text extraction\n",
    "        else:\n",
    "            failed_links.append(link) # Keep track of failures for reporting\n",
    "\n",
    "    print(f\"{GREEN}Fetched: {len(content)}{RESET}\")\n",
    "    print(f\"{RED}Failed: {len(failed_links)}{RESET}\")\n",
    "\n",
    "    if output_csv and content:\n",
    "        save_to_csv(content, output_csv)\n",
    "\n",
    "    return content, failed_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes_csv(all_links, output_file):\n",
    "    # Builds a 'nodes' CSV from an adjacency list (graph dictionary)\n",
    "\n",
    "    nodes = set(all_links.keys())  # Each key of the dictionnary \"all_links\" is a node (set() don't keep any possible duplicate)\n",
    "    \n",
    "    for targets in all_links.values():\n",
    "        nodes.update(targets)  # Each value of the dictionnary is a node\n",
    "\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:  # Create (or overwrite) a csv file \n",
    "        writer = csv.writer(f)  # Create an object that will enable our code to write within the file\n",
    "        writer.writerow([\"node_id\"])\n",
    "\n",
    "        for node in nodes:\n",
    "            writer.writerow([node])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges_csv(links, output_file):\n",
    "    # Builds an 'edges' CSV from an adjacency list (directed graph)\n",
    "    \n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"source\", \"target\"])  # We start by writing the headers of our column\n",
    "\n",
    "        for source, targets in links.items():\n",
    "            for target in targets:  \n",
    "                if target != source:  # Avoid self-loops (A -> A)\n",
    "                    writer.writerow([source, target])  # If the target is different form the source (not A -> A), we add the relation in our file\n",
    "                else :\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581d647",
   "metadata": {},
   "source": [
    "## Configuration and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    {\"name\": \"MBG\",\n",
    "     \"start_url\": \"https://www.mindbodygreen.com/\",\n",
    "     \"crawler_func\": crawl_MBG_seed_links}, # We write the specific function without calling it ()\n",
    "\n",
    "    {\"name\": \"wiki\",\n",
    "     \"start_url\": wiki_seed_pages,\n",
    "     \"crawler_func\": crawl_wiki_seed_links}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"========================== SOURCES PIPELINE ==========================\"\"\"\n",
    "\n",
    "\"\"\"=== SETUP ===\"\"\"\n",
    "for source in sources:\n",
    "    source_name = source[\"name\"]\n",
    "    start_point = source[\"start_url\"]\n",
    "\n",
    "    files = get_filenames(source_name) # Build all standardized output paths for this source \n",
    "\n",
    "    print(f\"=== SOURCE PROCESSING : {source_name} ===\")\n",
    "    \n",
    "    print(\"\\n=== CRAWLING ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Starting the crawling...{RESET}\")    \n",
    "    crawled_links, dico_links = source[\"crawler_func\"](start_point) # Ex : crawled_links = crawl_MBG_seed_links(https://....)\n",
    "    print(f\"{GREEN}-> Found {len(crawled_links)} links{RESET}\")\n",
    "    \n",
    "    print(\"\\n=== SCRAPING ===\")\n",
    "    \n",
    "    print(f\"{BLUE}[{source_name}] Starting the scraping...{RESET}\")\n",
    "    get_html_content((crawled_links), files['raw']) # Fetch HTML for each crawled URL and save it as a CSV (url, html)\n",
    "    print(f\"{GREEN}Raw HTML data saved to {files['raw']}{RESET}\")    \n",
    "\n",
    "    print(\"\\n=== CREATE CSV OF NODES ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Creating nodes CSV...{RESET}\")\n",
    "    create_nodes_csv(dico_links, files['nodes']) # We convert the list of links into a dictionary {url: []} to be compatible\n",
    "    print(f\"{GREEN}Nodes CSV saved to {files['nodes']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== CREATE CSV OF EDGES ===\\n\")\n",
    "    print(f\"{BLUE}[{source_name}] Creating edges CSV...{RESET}\")\n",
    "    create_edges_csv(dico_links, files['edges']) # Export directed edges (source -> target) from the adjacency list\n",
    "    print(f\"{GREEN}Edges CSV saved to {files['edges']}{RESET}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600f111",
   "metadata": {},
   "source": [
    "## **Data Collection of Feedspot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9164ec2",
   "metadata": {},
   "source": [
    "## Collection of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_topic_words_csv = pd.read_csv(\"data/main_topic_words_filtered.csv\")\n",
    "main_topic_words_str = main_topic_words_csv.iloc[0, 0]\n",
    "main_topic_words = ast.literal_eval(main_topic_words_str)\n",
    "print(main_topic_words)\n",
    "print(len(main_topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfb55b",
   "metadata": {},
   "source": [
    "## Definitions implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace143a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links, main_topic_words=None):\n",
    "    if main_topic_words is None:  # If no list of required keywords is given then we create an empty one\n",
    "        main_topic_words = []\n",
    "        \n",
    "    filtered = []\n",
    "    \n",
    "    for link in links:\n",
    "        l_lower = link.lower()  # Transformation of capital letter into lower case letter\n",
    "        \n",
    "        if main_topic_words and not any(word.lower() in l_lower for word in main_topic_words):  # The function skip the url if no required keywords are in the url\n",
    "            continue\n",
    "        filtered.append(link) # If the url past all the filter, it is added to the list of links\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feedspot_seed_pages(url, limit=60):\n",
    "    \n",
    "    soup = to_soup(url)\n",
    "    links_blogs = set()\n",
    "\n",
    "    if not soup:\n",
    "        print(f\"{RED}Error: Could not fetch main blogs page: {url}{RESET}\")\n",
    "        return set()\n",
    "\n",
    "    blogs = soup.find_all('a', href=True) # and tag.get('class') and 'wb-ba' in tag.get('class') and any('ext' in c for c in tag.get('class')))\n",
    "\n",
    "    for link in blogs:\n",
    "        href = link.get('href') #if link.name == 'a' else link.text.strip()  if href and \"http\" in href and \"bloggers.feedspot.com\" not in href:\n",
    "        \n",
    "        if not href.startswith('https?'):\n",
    "            href = urljoin(url, href)\n",
    "\n",
    "        if urlparse(href).netloc != urlparse(url).netloc:\n",
    "            if any(filter in href for filter in filters_noise):\n",
    "                continue\n",
    "\n",
    "            if \"mindbodygreen.com\" in href:\n",
    "                 continue\n",
    "            \n",
    "            if href and \"http\" in href and \"feedspot\" not in href :\n",
    "                links_blogs.add(href)\n",
    "\n",
    "            if len(links_blogs) >= limit: \n",
    "                break\n",
    "    \n",
    "    return sorted(links_blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b75313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_breadth_first_feedspot(url, max_levels=1, main_topic_words=main_topic_words): \n",
    "    queue = deque([(url, 0)])\n",
    "    visited_links = [url]  \n",
    "    last_level_links = []\n",
    "    dico = {}\n",
    "\n",
    "    while queue:\n",
    "        current_url, current_level = queue.popleft()\n",
    "\n",
    "        if current_level == max_levels:\n",
    "            if current_url not in last_level_links:\n",
    "                last_level_links.append(current_url)\n",
    "            continue\n",
    "        elif current_level > max_levels:\n",
    "            continue\n",
    "    \n",
    "        soup = to_soup(current_url)\n",
    "        \n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        dico.setdefault(current_url, [])\n",
    "\n",
    "        content_area = soup.body \n",
    "        \n",
    "        if not content_area:\n",
    "            continue\n",
    "        \n",
    "        links = sorted(content_area.find_all('a', href=True), key=lambda x: x.get('href'))\n",
    "        \n",
    "        for item in links :\n",
    "            href = item.get('href')\n",
    "\n",
    "            full_url = urljoin(current_url, href)\n",
    "            full_url = full_url.split('?')[0].split('#')[0]\n",
    "            full_url = full_url.lower()\n",
    "\n",
    "            if main_topic_words and not any(word.lower() in full_url for word in main_topic_words):\n",
    "                continue\n",
    "            \n",
    "            if any(filter in full_url for filter in filters_noise):\n",
    "                continue\n",
    "\n",
    "            dico[current_url].append(full_url)\n",
    "\n",
    "            if full_url not in visited_links:\n",
    "                visited_links.append(full_url)\n",
    "                queue.append((full_url, current_level + 1))\n",
    "                    \n",
    "    return visited_links, last_level_links, dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31474a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_feedspots_seed_links(urls, main_topic_words=None):\n",
    "    if main_topic_words is None:\n",
    "        main_topic_words = []\n",
    "\n",
    "    topic_links = []\n",
    "    seen_links = set()\n",
    "    global_dico = {}\n",
    "\n",
    "    for url in list(urls):                                                              \n",
    "        print(\"Crawling the url:\", url)\n",
    "\n",
    "        # Definition of our parameters for Feedspots\n",
    "        MAX_LINKS_TO_DFS = 10\n",
    "        MAX_BREADTH_LEVEL = 1\n",
    "        MAX_DEPTH = 1\n",
    "        MAX_LINKS_PER_PAGE = 15\n",
    "\n",
    "        # Crawl en largeur\n",
    "        breadth_links, last_level_links, dico = extract_links_breadth_first_feedspot(url, max_levels=MAX_BREADTH_LEVEL)\n",
    "\n",
    "        # Crawl en profondeur à partir des liens du dernier niveau\n",
    "        depth_links = extract_links_depth_from_list(max_links_to_dfs=MAX_LINKS_TO_DFS,\n",
    "                                                start_links=last_level_links, \n",
    "                                                visited_links=breadth_links.copy(), \n",
    "                                                dico=dico, max_depth=MAX_DEPTH, \n",
    "                                                max_links_per_page=MAX_LINKS_PER_PAGE)\n",
    "        \n",
    "        for k, v in dico.items(): \n",
    "            global_dico.setdefault(k, []) \n",
    "            global_dico[k].extend(v)\n",
    "\n",
    "        # Retourne tous les liens uniques sous forme de liste (déjà fait pour le BFS mais pas pour le DFS)\n",
    "        all_links = set(breadth_links) | set(depth_links)\n",
    "        link_filtered = filter_links(list(all_links), main_topic_words=main_topic_words)\n",
    "        \n",
    "        for link in link_filtered :\n",
    "            \n",
    "            if link not in seen_links :\n",
    "                topic_links.append(link)\n",
    "                seen_links.add(link)\n",
    "\n",
    "    return topic_links, global_dico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490fd2e",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"========================== SOURCES PIPELINE ==========================\"\"\"\n",
    "source_name = \"blogs\"\n",
    "start_point = \"https://bloggers.feedspot.com/women_lifestyle_blogs/?_src=bloggers_directory\"\n",
    "\n",
    "files = get_filenames(source_name) # Build all standardized output paths for this source \n",
    "\n",
    "print(f\"=== SOURCE PROCESSING : {source_name} ===\")\n",
    "\n",
    "print(\"\\n=== CRAWLING ===\")\n",
    "\n",
    "print(f\"{BLUE}[{source_name}] Starting the crawling...{RESET}\")   \n",
    "seed_blogs = extract_feedspot_seed_pages(start_point, limit=60)\n",
    "print(f\"{BLUE}[{source_name}] Links to crawl: {RESET}\",len(seed_blogs))  \n",
    "crawled_links, dico_links = crawl_feedspots_seed_links(list(seed_blogs), main_topic_words=main_topic_words)\n",
    "print(f\"{GREEN}-> Found {len(crawled_links)} links{RESET}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== SCRAPING ===\")\n",
    "\n",
    "print(f\"{BLUE}[{source_name}] Starting the scraping...{RESET}\")\n",
    "get_html_content((crawled_links), files['raw']) # Fetch HTML for each crawled URL and save it as a CSV (url, html)\n",
    "print(f\"{GREEN}Raw HTML data saved to {files['raw']}{RESET}\")    \n",
    "\n",
    "\n",
    "print(\"\\n=== CREATE CSV OF NODES ===\")\n",
    "\n",
    "print(f\"{BLUE}[{source_name}] Creating nodes CSV...{RESET}\")\n",
    "create_nodes_csv(dico_links, files['nodes']) # We convert the list of links into a dictionary {url: []} to be compatible\n",
    "print(f\"{GREEN}Nodes CSV saved to {files['nodes']}{RESET}\")\n",
    "\n",
    "print(\"\\n=== CREATE CSV OF EDGES ===\\n\")\n",
    "print(f\"{BLUE}[{source_name}] Creating edges CSV...{RESET}\")\n",
    "create_edges_csv(dico_links, files['edges']) # Export directed edges (source -> target) from the adjacency list\n",
    "print(f\"{GREEN}Edges CSV saved to {files['edges']}{RESET}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
