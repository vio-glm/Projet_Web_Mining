{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c740bc4",
   "metadata": {},
   "source": [
    "# **Data Collection**\n",
    "### Here below is the first step of our projet, the code for our data collection. We've decided to scrap Wikipedia and Feedspots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6301088",
   "metadata": {},
   "source": [
    "##### We'll start by listing all our import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc36d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#nltk.download('punkt_tab') #: to uncomment if not already downloaded\n",
    "#nltk.download('stopwords') #: to uncomment if not already downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34629ac1",
   "metadata": {},
   "source": [
    "#### Below is a list of useful fonctions that will be used during the data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33650b66",
   "metadata": {},
   "source": [
    "##### The functions belows are used for the collect of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c06a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_verify_url(url) :\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}  \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:  # If the status code is not OK (200) the function return none and an error message\n",
    "            print(f\"Failed to fetch the url: {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        return response  \n",
    "    except requests.RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99499234",
   "metadata": {},
   "source": [
    "This function is used to safely retrieve the HTML content of a web page. It sends an HTTP request to a given URL while mimicking a real web browser by specifying a User-Agent header, which helps avoid blocking by some websites. The function then checks whether the server responds with a successful status code (200). If the request fails, the page is inaccessible, or any network error occurs, the function returns None. Otherwise, it returns the raw HTTP response containing the page’s HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4067bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_soup(url):\n",
    "    response = fetch_verify_url(url)\n",
    "    if response:  # If the response is not none, the function return the beautiful soup object\n",
    "        return BeautifulSoup(response.text, 'html.parser') \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f511a5f",
   "metadata": {},
   "source": [
    "This function converts the HTML content of a web page into a structured object that can be easily analyzed. It first calls the fetch_verify_url function to retrieve the web page’s HTML content. If the request is successful, the HTML is parsed using BeautifulSoup with the built-in HTML parser, producing a navigable representation of the page. If the page cannot be retrieved, the function returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ecc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links, required_keywords=None, domain=None, already_seen=None):\n",
    "    if required_keywords is None:  # If no list of required keywords is given then we create an empty one\n",
    "        required_keywords = []\n",
    "    if already_seen is None:  # If no list of already seen links is given then we create an empty one\n",
    "        already_seen = set()\n",
    "    \n",
    "    filtered = []\n",
    "    for l in links:\n",
    "        l_lower = l.lower()  # Transformation of capital letter into lower case letter\n",
    "        if required_keywords and not any(keyword.lower() in l_lower for keyword in required_keywords):  # The function skip the url if no required keywords are in the url\n",
    "            continue\n",
    "        if domain and urlparse(l).netloc != domain:  # The function filter the links that are not in the domain\n",
    "            continue\n",
    "        if l in already_seen:  # The function filter the links already in the list of links\n",
    "            continue\n",
    "        filtered.append(l) # If the url past all the filter, it is added to the list of links\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56804124",
   "metadata": {},
   "source": [
    "This function filters a list of URLs in order to keep only the most relevant web pages. It removes links that doesn't contain predefined required keywords. The function can also restrict the results to a specific domain, allowing only internal links to be kept. Additionally, it eliminates URLs that have already been encountered, preventing duplicates during the crawling process.\n",
    "\n",
    "By applying these filters, the function helps reduce noise and improves the quality and relevance of the collected dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6121a",
   "metadata": {},
   "source": [
    "The functions below are used to collect the corpus of web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_corpus(links):\n",
    "    corpus = []\n",
    "\n",
    "    for link in links:\n",
    "        response = fetch_verify_url(link)\n",
    "        if response:\n",
    "            corpus.append({'url': link, 'html': response.text}) # A dictionary with the url (as the key) and the corpus is created\n",
    "        time.sleep(random.uniform(1,4))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed37f7e",
   "metadata": {},
   "source": [
    "This function collects the raw HTML content of a list of web pages.\n",
    "It iterates over each URL in the input list, sends an HTTP request using the fetch_verify_url function, and checks whether the request was successful.\n",
    "If a valid response is received, the function stores the page URL and its HTML source code as a dictionary and appends it to the corpus list.\n",
    "A random delay between 1 and 4 seconds is added after each request to avoid overloading the server and to reduce the risk of being blocked.\n",
    "Finally, the function returns the complete corpus containing the HTML content of all successfully fetched pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1453ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"Error : There is no data to save\")\n",
    "        return\n",
    "\n",
    "    fieldnames = data[0].keys()  # Detection of the existing colons in the data file\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:  # Opens the csv file as utf-8\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)  # Initialising a writer to write the dictionary into the csv file\n",
    "        writer.writeheader()  # writes the colons headers\n",
    "        writer.writerows(data)  # writes the rows\n",
    "\n",
    "    print(f\"CSV saved : {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f7521",
   "metadata": {},
   "source": [
    "This function saves a list of dictionaries into a CSV file.\n",
    "It first checks whether the input data is empty; if no data is provided, the function prints an error message and stops execution.\n",
    "The column names of the CSV file are automatically extracted from the keys of the first dictionary in the list.\n",
    "The function then opens (or creates) the CSV file in UTF-8 encoding and initializes a DictWriter to correctly map dictionary keys to CSV columns.\n",
    "The header row is written first, followed by all rows of data.\n",
    "Finally, a confirmation message is printed to indicate that the CSV file has been successfully saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f63581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    for tag in soup(['script', 'style', 'noscript']):  # Supress any unessecary tags\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=' ', strip=True)  # Collect all visible text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supress any unecessary spaces\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9cc3a",
   "metadata": {},
   "source": [
    "This function cleans raw HTML content and extracts only the meaningful textual information.\n",
    "First, the HTML string is parsed into a BeautifulSoup object, which allows structured navigation of the document.\n",
    "All non-textual and irrelevant elements such as script, style, and noscript tags are then removed to avoid including code or hidden content in the final text.\n",
    "The function extracts all visible text from the cleaned HTML, using spaces as separators and trimming unnecessary leading and trailing whitespace.\n",
    "Finally, multiple consecutive spaces are reduced to a single space to produce a clean and readable text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'html' not in df.columns:  # Verify that the html colon exists\n",
    "        raise ValueError(f\"The html colon is missing in: {input_csv}\")\n",
    "\n",
    "    df['cleaned_text'] = df['html'].apply(clean_html)  # Cleans the html colon\n",
    "    df = df[['url', 'cleaned_text']]  # Keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # Creats a new csv file as the output of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee12b1",
   "metadata": {},
   "source": [
    "This function cleans an entire CSV file containing raw HTML content.\n",
    "First, the CSV file is loaded into a pandas DataFrame, allowing efficient column-wise processing.\n",
    "The function checks whether the column named html exists; if it is missing, an error is raised to prevent silent failures and ensure data consistency.\n",
    "Each HTML document in the html column is then cleaned using the clean_html function, and the resulting plain text is stored in a new column called cleaned_text.\n",
    "Only the relevant columns (url and cleaned_text) are retained, discarding the original raw HTML to reduce file size and complexity.\n",
    "Finally, the cleaned data is saved into a new CSV file, producing a structured and reusable text corpus for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_html(text):\n",
    "    text = text.lower()  # convert all letters to lowercase\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers like [1], [2], etc.\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # keep only English letters, numbers, and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with a single space\n",
    "    return text.strip()  # remove leading and trailing spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe909d93",
   "metadata": {},
   "source": [
    "This function performs text normalization on cleaned HTML content in order to prepare it for linguistic analysis.\n",
    "First, all characters are converted to lowercase to ensure case-insensitive processing and avoid treating the same word as different tokens.\n",
    "Reference markers such as [1], [2], commonly found in Wikipedia-style pages, are removed using a regular expression.\n",
    "All characters that are not lowercase English letters, digits, or spaces are then replaced with spaces, effectively removing punctuation and special symbols.\n",
    "Multiple consecutive spaces are collapsed into a single space to produce a cleaner and more consistent text format.\n",
    "Finally, leading and trailing spaces are removed before returning the normalized text, ensuring uniform formatting across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf222dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the cleaned text as the input\n",
    "\n",
    "    if 'cleaned_text' not in df.columns:  # Verify that the cleaned text colon exists\n",
    "        raise ValueError(f\"The cleaned text is missing in: {input_csv}\")\n",
    "\n",
    "    df['normalized_text'] = df['cleaned_text'].apply(normalize_html)  # normalize the cleaned text\n",
    "    df = df[['url', 'normalized_text']]  # keep the url and normalized text \n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # creats a new csv file as the output of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b39d81",
   "metadata": {},
   "source": [
    "This function applies text normalization to an entire CSV file containing cleaned textual data.\n",
    "First, the input CSV file is loaded into a pandas DataFrame. The function then checks whether the column cleaned_text exists to ensure that the expected input data is available. If the column is missing, a descriptive error is raised to prevent silent failures.\n",
    "The normalization process is applied to each row of the cleaned_text column using the normalize_html function, producing a new column called normalized_text.\n",
    "Only the URL and the normalized text columns are retained, as the raw and intermediate data are no longer needed at this stage.\n",
    "Finally, the resulting DataFrame is saved to a new CSV file, which serves as the normalized version of the corpus and can be reused in subsequent processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63529178",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english'))) + [\"'s\"]\n",
    "stem = nltk.stem.SnowballStemmer(\"english\")\n",
    "def tokenize_html(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]  # remove punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # remove stopwords\n",
    "    tokens = [stem.stem(token) for token in tokens]  # apply stemming (racinisation)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb107ee",
   "metadata": {},
   "source": [
    "This part of the code prepares and tokenizes normalized text for linguistic analysis.\n",
    "First, a list of English stopwords is created using NLTK’s predefined stopword list, with the additional removal of the possessive form ’s, which often appears in English texts but carries little semantic value.\n",
    "A Snowball stemmer configured for English is then initialized to reduce words to their root form.\n",
    "\n",
    "The tokenize_html function takes a text string as input and converts it to lowercase to ensure consistency. The text is then tokenized into individual words using NLTK’s tokenizer.\n",
    "Punctuation tokens are removed, followed by the removal of stopwords to keep only meaningful terms.\n",
    "Finally, stemming is applied to each remaining token, reducing inflected or derived words to a common base form.\n",
    "The function returns a list of processed tokens, which can be directly used for further tasks such as frequency analysis, topic modeling, or vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the normalized text as the input\n",
    "\n",
    "    if 'normalized_text' not in df.columns:  # Verify that the normalized text colon exists\n",
    "        raise ValueError(f\"The normalized text is missing in: {input_csv}\")\n",
    "\n",
    "    df['tokenized_text'] = df['normalized_text'].apply(tokenize_html)  # cleans the html colon\n",
    "    df = df[['url', 'tokenized_text']]  # keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # creats a new csv file as the output of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05971d5",
   "metadata": {},
   "source": [
    "This function applies tokenization to a CSV file containing normalized text.\n",
    "It begins by loading the input CSV file into a pandas DataFrame. Before processing, the function checks whether the column normalized_text exists; if not, it raises an error to prevent incorrect execution of the pipeline.\n",
    "\n",
    "The function then applies the tokenize_html function to each row of the normalized_text column. This step transforms each text into a list of cleaned and stemmed tokens.\n",
    "Only the URL and the resulting tokenized text are kept in the final DataFrame to reduce unnecessary data.\n",
    "\n",
    "Finally, the processed data is saved into a new CSV file, producing a structured and reusable representation of the tokenized corpus that can be used for further text analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef9b41",
   "metadata": {},
   "source": [
    "##### Following that, we can start the data collection with the scraping of Feedspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "100 blogs has been found\n",
      "Blogs founded: ['https://www.mindbodygreen.com/', 'https://www.thepioneerwoman.com/', 'https://goop.com/', 'https://www.artofmanliness.com/', 'https://www.themarthablog.com/', 'https://sincerelyjules.com/', 'https://www.pbfingers.com/', 'https://camillestyles.com/', 'https://cupofjo.com/', 'https://www.theskinnyconfidential.com/', 'https://www.apetogentleman.com/', 'https://www.primermagazine.com/', 'https://livinginyellow.com/', 'https://www.ahealthysliceoflife.com/', 'https://onbetterliving.com/', 'https://thestripe.com/', 'https://helloadamsfamily.com/', 'https://julieblanner.com/', 'https://heleneinbetween.com/', 'https://inspirationsandcelebrations.net/', 'https://www.elizabethrider.com/', 'https://www.katiedidwhat.com/', 'https://www.idyllicpursuit.com/', 'https://witwhimsy.com/', 'https://happilyevaafter.com/', 'https://theblueridgegal.com/', 'https://lmgfl.com/', 'https://simplytaralynn.com/', 'https://socialifestylemag.com/', 'https://www.lizbreygel.com/', 'https://www.beautycookskisses.com/', 'https://freshlandmag.com/', 'https://witanddelight.com/blog/', 'https://www.theblondeabroad.com/travel-blog/', 'https://middleagedmama.com.au/', 'https://www.beccatilleyblog.com/', 'https://theperennialstyle.com/', 'https://quintessenceblog.com/whats-new/', 'https://meetat-thebarre.com/', 'https://hejdoll.com/', 'https://iamaileen.com/blog/', 'https://carlyriordan.com/', 'https://alysonhaley.com/', 'https://treasuresandtravelsblog.com/blog/', 'https://www.predupre.com/', 'https://www.lifeoftrends.com/lifestyle', 'https://lifegram.org/', 'https://livingswag.com/', 'https://sweethumblehome.com/', 'https://richu1893.wordpress.com/', 'https://50plano.com/', 'https://lifestyleonabudget.com/category/tips-tricks/', 'https://heleneinbetween.com/blog', 'https://corporette.com/category/lifestyle/', 'https://positivelypresent.com/archive', 'https://theneguide.com/', 'https://flow-spirit.com/lifestyle-blogs/', 'https://terilynadams.com/', 'https://www.sazan.me/blog', 'https://www.thesundaysnug.com/', 'https://belleharris.com/blogs/news', 'https://iamkelib.com/blog/', 'https://an-ideal-life.com/category/an-ideal-life/', 'https://ourbloglife.com/', 'https://jaceyoutwest.com/the-blog/', 'https://abeautifulmess.com/', 'https://www.zoella.co.uk/', 'https://www.consciouslifestylemag.com/', 'https://pumpsandiron.com/', 'https://www.thirteenthoughts.com/', 'https://www.ferbena.com/', 'https://www.cluburb.com/', 'https://blog.justinablakeney.com/', 'https://www.superhitideas.com/', 'https://crystalinmarie.com/', 'https://scoutthecity.com/', 'https://amelialiana.com/', 'https://www.knowseeker.com/', 'https://hollybeetells.com/', 'https://loveandloathingla.com/', 'https://www.dallaswardrobe.com/', 'https://joylynnlifestyle.com/', 'https://zerxza.com/', 'https://www.dailysweetness.com/', 'https://www.mrglitterati.com/', 'https://dawnandhope.com/', 'https://freshexchange.com/blog/', 'https://www.sofiaclara.com/journal', 'https://madeathomecrafts.com/', 'https://lifestyleandberries.com/category/lifestyle/', 'https://www.treastblog.com/', 'https://peplifestyle.com/luxury-lifestyle/', 'http://www.mrpogitips.com/', 'https://www.nearlyallthings.com/', 'https://hookonlook.com/category/life-style/', 'https://www.julesacree.com/all', 'https://goddesslikebody.com/', 'https://blueperkmoment.com/', 'https://bushraslifestyle.com/blog/', 'https://meoneverything.blog/']\n"
     ]
    }
   ],
   "source": [
    "url_blogs = \"https://bloggers.feedspot.com/lifestyle_blogs/\"\n",
    "\n",
    "soup = to_soup(url_blogs)\n",
    "\n",
    "if not soup:\n",
    "    print(\"Error: Could not fetch main blogs page:\", url_blogs)\n",
    "\n",
    "blogs = soup.find_all(lambda tag: tag.name in ['a', 'span'] and tag.get('class') and 'wb-ba' in tag.get('class') and any('ext' in c for c in tag.get('class')))\n",
    "\n",
    "links_blogs = []\n",
    "for item in blogs:\n",
    "    href = item.get('href') if item.name == 'a' else item.text.strip()\n",
    "    if href and \"http\" in href and \"bloggers.feedspot.com\" not in href:  # Exclusion of internal links to only extract links directing to blogs\n",
    "        links_blogs.append(href)\n",
    "\n",
    "print(len(links_blogs), \"blogs has been found\")\n",
    "print(\"Blogs founded:\", links_blogs[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba33f5",
   "metadata": {},
   "source": [
    "This code retrieves a curated list of lifestyle blogs from the Feedspot website. It first loads and parses the HTML content of the Feedspot page containing the blog rankings. If the page cannot be accessed, the program stops to prevent further errors. The code then identifies the HTML elements corresponding to blog links by targeting specific tags and CSS classes used by Feedspot.\n",
    "\n",
    "For each identified element, the script extracts the blog URL and filters out internal Feedspot links, keeping only external blog websites. The resulting list contains the URLs of lifestyle blogs, which serve as the starting points (seed URLs) for the subsequent crawling process. Finally, the script reports the number of blogs successfully collected and displays a sample of the extracted URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_blog(url):\n",
    "    soup = to_soup(url)\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    links = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        # gérer les liens relatifs (/about → https://blog.com/about)\n",
    "        full_url = urljoin(url, href)\n",
    "        if full_url.startswith(\"http\") and full_url not in links:\n",
    "            links.append(full_url)\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b507c8",
   "metadata": {},
   "source": [
    "This function extracts all hyperlinks from a given web page. It first retrieves and parses the HTML content of the page using the to_soup function. If the page cannot be accessed, the function returns None. The function then scans all anchor (<a>) elements and extracts their href attributes. Relative URLs are converted into absolute URLs using the base page URL to ensure consistency.\n",
    "\n",
    "Only valid HTTP links are kept, and duplicate URLs are removed. The function returns a list of unique hyperlinks found on the page, which are later used in the crawling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b972d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclude_keywords = [\n",
    "    \"privacy\",       \n",
    "    \"contact\",      \n",
    "    \"terms\",       \n",
    "    \"login\",      \n",
    "    \"signup\",      \n",
    "    \"register\",    \n",
    "    \"tag\",       \n",
    "    \"category\",   \n",
    "    \"archive\",       \n",
    "    \"feed\",     \n",
    "    \"comments\",    \n",
    "    \"search\",        \n",
    "    \"newsletter\",    \n",
    "    \"cart\",          \n",
    "    \"checkout\",     \n",
    "    \"admin\",        \n",
    "    \"wp-\",  \n",
    "    \"cgi-bin\",\n",
    "    \"privacy-policy\",\n",
    "    \"cookie\",\n",
    "    \"sitemap\",\n",
    "    \"login.php\", \n",
    "    \"register.php\", \n",
    "    \"unsubscribe\", \n",
    "    \"terms-of-service\",\n",
    "    \"press\",    \n",
    "    \"ad\",   \n",
    "    \"ads\",  \n",
    "    \"advertisement\", \n",
    "    \"accessibility\",  \n",
    "    \"sponsor\",\n",
    "    \"disclaimer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0edfcc",
   "metadata": {},
   "source": [
    "A list of exclusion keywords is defined to remove non-relevant web pages from the crawling process. These keywords correspond to administrative, legal, technical, or commercial pages such as privacy policies, contact forms, login pages, archives, tags, advertisements, and sponsored content. By filtering out URLs containing these terms, the crawler focuses on pages that are more likely to contain meaningful blog content, reducing noise in the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_in_blogs_1 = {}\n",
    "\n",
    "total_new_urls_1 = 0\n",
    "\n",
    "for url in links_blogs[:2]:  # scrappe les x premiers blogs et retourne le nombre de lien trouvé sur la page d'accueil et les 5 premiers liens\n",
    "    print(\"\\nLinks from the blog (first round):\", url)\n",
    "\n",
    "    links = get_links_from_blog(url)\n",
    "    if links is None:\n",
    "        print(\"→ 0 links found (scraping failed or blocked)\")\n",
    "        links_in_blogs_1[url] = []\n",
    "        continue\n",
    "\n",
    "    domain = urlparse(url).netloc\n",
    "    filtered_links = filter_links(links, exclude_keywords=exclude_keywords, domain=domain, already_seen=set())\n",
    "\n",
    "    links_in_blogs_1[url] = filtered_links\n",
    "\n",
    "    total_new_urls_1 += len(filtered_links)\n",
    "\n",
    "    print(\"→\", len(filtered_links), \"links found:\", filtered_links[:1])\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"Total new URLs found in this first round iteration:\", total_new_urls_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e97b1",
   "metadata": {},
   "source": [
    "In the first crawling round, the script visits the homepages of a subset of selected blogs and extracts all hyperlinks present on each page. The collected links are then filtered using predefined exclusion keywords to remove non-content pages. Additionally, the filtering process is restricted to the same domain as the source blog in order to retain only internal links. The resulting URLs are stored in a dictionary and counted to measure the number of relevant pages identified during this initial crawling stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
