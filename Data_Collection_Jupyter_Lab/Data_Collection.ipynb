{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648a76d6",
   "metadata": {},
   "source": [
    "IMPORTS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd99e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, deque\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re \n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import time\n",
    "import token \n",
    "from urllib.parse import urljoin, urlparse, unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "540f5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Required for new versions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')   # Required for the lemmatiser\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ac71",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c7a5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF COLORS ---\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8001b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF HEADER ---\n",
    "\n",
    "HEADER_BROWSER = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\"}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADER_BROWSER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb002d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF FILTERS ---\n",
    "\n",
    "filters_noise = [\n",
    "    # E-commerce\n",
    "    \"/product\", \"/products\", \"/shop\", \"/store\", \"/cart\", \"/checkout\",\n",
    "    # User\n",
    "    \"/account\", \"/login\", \"/register\", \"/profile\", \n",
    "    # Legal and Administrative pages\n",
    "    \"/about\", \"/contact\", \"/privacy\", \"/terms\", \"/policy\", \"/legal\", \"/accessibility\", \n",
    "    \"/editorial-process\", \"/data-collection\", \"/disclaimer\", \"/cookies\", \"/sponsor\",\n",
    "    \"/advertise\", \"/jobs\", \"/faq\", \"/help\", \"/wc\", \n",
    "    # Site Structure\n",
    "    \"/tag\", \"/search\", \"/author\",\n",
    "    \"mailto:\", \"tel:\", \"javascript:\", \".jpg\", \".png\",\n",
    "    \"youtube.com\", \"instagram.com\", \"facebook.com\", \"twitter.com\", \"x.com\",\n",
    "    \"pinterest.com\", \"linkedin.com\", \"tiktok.com\", \"amazon.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe11fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF SEED LINKS FOR WIKIPEDIA ---\n",
    "\n",
    "wiki_seed_pages = [ # Our selection of a few pages = starting list\n",
    "    \"https://en.wikipedia.org/wiki/Lifestyle\",\n",
    "    \"https://en.wikipedia.org/wiki/Lifestyle_trends_and_media\",\n",
    "    \"https://en.wikipedia.org/wiki/Self-care\",\n",
    "    \"https://en.wikipedia.org/wiki/Physical_fitness\",\n",
    "    \"https://en.wikipedia.org/wiki/Healthy_diet\",\n",
    "    \"https://en.wikipedia.org/wiki/Travel\",\n",
    "    \"https://en.wikipedia.org/wiki/Outdoor_recreation\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_living\",\n",
    "    \"https://en.wikipedia.org/wiki/Fashion\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1637a4",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a37e18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(source_name): \n",
    "    # Create all files from a source using the name specified in the source cell [21]\n",
    "    \n",
    "    directory_name = \"data\"\n",
    "    if not os.path.exists(directory_name): # Checks if the 'data' folder exists\n",
    "        os.makedirs(directory_name) # Setup the output directory\n",
    "        print(f\"{GREEN}Created directory : {directory_name}{RESET}\")\n",
    "    \n",
    "    # We assume that source_name contains no spaces. If changed, add : source_name = source_name.replace(\" \", \"_\")\n",
    "\n",
    "    return {\"raw\":       os.path.join(directory_name, f\"{source_name}_raw_html.csv\"),\n",
    "            \"corpus\":    os.path.join(directory_name, f\"{source_name}_corpus.csv\"),\n",
    "            \"cleaned\":   os.path.join(directory_name, f\"{source_name}_cleaned.csv\"),\n",
    "            \"norm\":      os.path.join(directory_name, f\"{source_name}_corpus_norm.csv\"),\n",
    "            \"tokenized\": os.path.join(directory_name, f\"{source_name}_corpus_tokenized.csv\"),\n",
    "            \"nodes\" :    os.path.join(directory_name, f\"{source_name}_nodes.csv\"),\n",
    "            \"edges\" :    os.path.join(directory_name, f\"{source_name}_edges.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f52e9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url) :\n",
    "    # Fetches the content of a URL\n",
    "    \n",
    "    time.sleep(random.uniform(1, 3)) # Random delay between 1 and 3 seconds to avoid overloading the server\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, timeout=10) # Timeout set to 10s to prevent the crawler from hanging on slow pages\n",
    "        if response.status_code == 200: \n",
    "            return response # If the status code is not OK (200), the function returns none and an error message\n",
    "        else:\n",
    "            print(f\"Failed to fetch the url: {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    except requests.RequestException: # Catch network-related errors (DNS failure, connection refused, etc.)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98a2c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_soup(url): \n",
    "    # Parses the HTML content of a URL into a BeautifulSoup object\n",
    "    \n",
    "    response = fetch_url(url) \n",
    "\n",
    "    if response: # If the fetch was successful, parse the HTML\n",
    "        return BeautifulSoup(response.text, 'html.parser') \n",
    "    else: # If the response is not none, the function return the beautiful soup object\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae7d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_breadth_first(url, max_levels=1): \n",
    "    # Crawls a website using Breadth-First Search (BFS)\n",
    "\n",
    "    queue = deque([(url, 0)]) # Initialize BFS queue with (url, current_depth)\n",
    "    visited_links = [url]  # Track visited URLs to avoid duplicates (list preserves order)\n",
    "    last_level_links = [] # Store links found at the maximum level\n",
    "    dico = {} # For the graph structure {parent : [child1, child2]}\n",
    "\n",
    "    while queue:\n",
    "        current_url, current_level = queue.popleft()\n",
    "\n",
    "        if current_level == max_levels: # if the limit is reached, we store the links in the list\n",
    "            if current_url not in last_level_links:\n",
    "                last_level_links.append(current_url)\n",
    "            continue\n",
    "        elif current_level > max_levels:\n",
    "            continue\n",
    "    \n",
    "        soup = to_soup(current_url) # Fetching and parsing\n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        dico.setdefault(current_url, [])\n",
    "\n",
    "        content_area = soup.body # Broad extraction scope: analysis of the entire <body> to retrieve as many links as possible\n",
    "        if not content_area:\n",
    "            continue\n",
    "\n",
    "        links = sorted(content_area.find_all('a', href=True), key=lambda x: x.get('href')) # Extract all 'a' tags with an 'href' attribute and sort alphabetically\n",
    "        \n",
    "        for item in links :\n",
    "            href = item.get('href') # href is short for Hypertext REFerence. It indicates the destination of the link\n",
    "            full_url = urljoin(current_url, href)\n",
    "            full_url = full_url.split('?')[0].split('#')[0] # Cut at the \"?\" of \"#\" and keep only the beginning : [0]\n",
    "            full_url = full_url.lower()\n",
    "\n",
    "            if any(filter in full_url for filter in filters_noise):\n",
    "                continue\n",
    "            \n",
    "            dico[current_url].append(full_url) # Add edge to the adjacency list (Parent -> Child)\n",
    "\n",
    "            if full_url not in visited_links: # If the link is new\n",
    "                visited_links.append(full_url) \n",
    "                queue.append((full_url, current_level + 1)) # Add it to the queue for future crawling\n",
    "                    \n",
    "    return visited_links, last_level_links, dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8070df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_depth_first(start_url, visited_links, dico, max_depth=3, max_links_per_page=10, current_depth=0): \n",
    "    #Crawls a website using Depth-First Search (DFS)\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "\n",
    "    soup = to_soup(start_url) # Fetching and parsing\n",
    "    if not soup :\n",
    "        return\n",
    "\n",
    "    dico.setdefault(start_url, []) # Ensure the current URL exists in the adjacency list\n",
    "    links_followed = 0 # Counter to max_links_per_page\n",
    "\n",
    "    content_area = soup.body # Broad extraction scope: analysis of the entire <body> to retrieve as many links as possible\n",
    "    if not content_area:\n",
    "        return\n",
    "    \n",
    "    links = sorted(content_area.find_all('a', href=True), key=lambda x: x.get('href')) # Extract all 'a' tags with an 'href' attribute and sort alphabetically\n",
    "    \n",
    "    for link in links:\n",
    "        if links_followed >= max_links_per_page: # Maximum number of links followed (crawled) from a given page\n",
    "            break\n",
    "\n",
    "        href = link.get('href')\n",
    "        full_url = urljoin(start_url, href) # Convert relative paths to absolute URLs\n",
    "        full_url = full_url.split('?')[0].split('#')[0] # Cut at the \"?\" of \"#\" and keep only the beginning : [0]\n",
    "        full_url = full_url.lower()\n",
    "\n",
    "        if any(filter in full_url for filter in filters_noise):\n",
    "                continue\n",
    "\n",
    "        dico[start_url].append(full_url) # Add edge to the adjacency list (Parent -> Child)\n",
    "\n",
    "        if full_url not in visited_links: # If the link is new, explore it deeply before moving to the next one\n",
    "            visited_links.append(full_url)\n",
    "            links_followed += 1\n",
    "\n",
    "            # Recursive call: dive deeper into this branch\n",
    "            extract_links_depth_first( start_url=full_url, visited_links=visited_links, dico=dico, max_depth=max_depth, max_links_per_page=max_links_per_page, current_depth=current_depth + 1)\n",
    "    \n",
    "    return visited_links, dico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89e5da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_depth_from_list(start_links, visited_links, dico, max_depth=3, max_links_per_page=10):\n",
    "    # Function to launch multiple DFS crawls from a list of seed URLs\n",
    "    \n",
    "    if visited_links is None:\n",
    "        visited_links = []\n",
    "    \n",
    "    sorted_links = sorted(start_links) # Sorted because same input order = same results\n",
    "    \n",
    "    for link in sorted_links:\n",
    "        if link not in visited_links:\n",
    "            visited_links.append(link) # Add the starting links to visited links\n",
    "\n",
    "    for start_url in sorted_links: # Maximum number of seeds (starting points) that we want to crawl in depth\n",
    "        extract_links_depth_first(start_url=start_url, visited_links=visited_links, \n",
    "                                  dico=dico, max_depth=max_depth, \n",
    "                                  max_links_per_page=max_links_per_page, current_depth=0)\n",
    "\n",
    "    return visited_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "182e3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_MBG_seed_links(url):\n",
    "    # Fixed parameters\n",
    "    \n",
    "    MAX_BREADTH_LEVEL = 2\n",
    "    MAX_DEPTH = 2\n",
    "    MAX_LINKS_PER_PAGE = 5 # Number of links per DFS\n",
    "\n",
    "    # Breadth-First Search (wide exploration)\n",
    "    breadth_links, last_level_links, dico = extract_links_breadth_first(url, max_levels=MAX_BREADTH_LEVEL)\n",
    "\n",
    "    # Depth-First Search from the BFS frontier\n",
    "    depth_links = extract_links_depth_from_list(start_links=last_level_links, \n",
    "                                                visited_links=breadth_links.copy(), \n",
    "                                                dico=dico, max_depth=MAX_DEPTH, \n",
    "                                                max_links_per_page=MAX_LINKS_PER_PAGE)\n",
    "\n",
    "    all_links = set(breadth_links) | set(depth_links) # Union of BFS and DFS results (order is not preserved, but duplicates are removed)\n",
    "    \n",
    "    return list(all_links), dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4380657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_links(url):\n",
    "    # Extracts internal Wikipedia article links from a given Wikipedia page\n",
    "    \n",
    "    wiki_links = set()\n",
    "\n",
    "    soup = to_soup(url) \n",
    "    if not soup:\n",
    "        return set() # Returns a list of URLs pointing to Wikipedia articles only\n",
    "\n",
    "    # Focus on : Main content paragraphs (article body)\n",
    "    main_content = soup.find_all('div', class_=\"mw-content-ltr mw-parser-output\")\n",
    "    for div in main_content:\n",
    "        for p in div.find_all('p'): # Target: paragraphs (<p>) within the class (main content div)\n",
    "            for item in p.find_all('a', href=True):\n",
    "                href = item.get('href')\n",
    "                if href.startswith(\"/wiki/\") and \":\" not in href: # Keep links \"/wiki/...\" and exclude pages containing \":\"\n",
    "                    wiki_links.add(urljoin(url, href))\n",
    "\n",
    "    # Focus on : \"See also\" section (related articles)\n",
    "    see_also = soup.find_all('div', class_='div-col')\n",
    "    for div in see_also:\n",
    "        for li in div.find_all('li'): # Target: lists (<li>) within columns ('div-col')\n",
    "            for item in li.find_all('a', href=True):\n",
    "                href = item.get('href')\n",
    "                if href.startswith(\"/wiki/\") and \":\" not in href:\n",
    "                    wiki_links.add(urljoin(url, href))\n",
    "\n",
    "    return list(wiki_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fdd31400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wiki_seed_links(wiki_seed_pages):\n",
    "    \n",
    "    wiki_all_links = set(wiki_seed_pages) # Set() avoids duplication, unlike lists\n",
    "    dico_wiki_links = {}\n",
    "    for url in wiki_seed_pages:\n",
    "        crawl_links = extract_wikipedia_links(url)\n",
    "        dico_wiki_links[url] = crawl_links\n",
    "        wiki_all_links.update(crawl_links) # Only add new links\n",
    "\n",
    "    return list(wiki_all_links), dico_wiki_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27f13838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"{RED}No data to save.{RESET}\")\n",
    "        return\n",
    "\n",
    "    # Warn if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{BLUE}Note: '{filename}' already exists and will be overwritten{RESET}\")\n",
    " \n",
    "    # Get the keys from the first dictionary for the CSV header from get_html_content\n",
    "    try:\n",
    "        fieldnames = data[0].keys() # Detection of the existing colons in the data file\n",
    "    \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f: # Opens the csv file as utf-8 (= character encoding system)\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL) # Initialising a writer to write the dictionary into the csv file\n",
    "            writer.writeheader() # Writes the colons headers\n",
    "            writer.writerows(data) # Writes the rows\n",
    "\n",
    "        print(f\"{GREEN}Success! Data saved to: {RESET}{os.path.abspath(filename)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{RED}Error saving file: {e}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92cd0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(links, output_csv, max=None):\n",
    "    # Fetches raw HTML content for a list of URLs\n",
    "\n",
    "    content = []\n",
    "    failed_links = []\n",
    "\n",
    "    for link in links[:max]:\n",
    "        response = fetch_url(link)\n",
    "        if response:\n",
    "            content.append({'url': link, 'html': response.text}) # Store raw HTML as text for text extraction\n",
    "        else:\n",
    "            failed_links.append(link) # Keep track of failures for reporting\n",
    "\n",
    "    print(f\"{GREEN}Fetched: {len(content)}{RESET}\")\n",
    "    print(f\"{RED}Failed: {len(failed_links)}{RESET}\")\n",
    "\n",
    "    if output_csv and content:\n",
    "        save_to_csv(content, output_csv)\n",
    "\n",
    "    return content, failed_links "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1288739",
   "metadata": {},
   "source": [
    "PrÃ©paration au link analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fbb37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes_csv(all_links, output_file):\n",
    "    # Builds a 'nodes' CSV from an adjacency list (graph dictionary)\n",
    "\n",
    "    nodes = set(all_links.keys())  # Each key of the dictionnary \"all_links\" is a node (set() don't keep any possible duplicate)\n",
    "    \n",
    "    for targets in all_links.values():\n",
    "        nodes.update(targets)  # Each value of the dictionnary is a node\n",
    "\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:  # Create (or overwrite) a csv file \n",
    "        writer = csv.writer(f)  # Create an object that will enable our code to write within the file\n",
    "        writer.writerow([\"node_id\"])\n",
    "\n",
    "        for node in nodes:\n",
    "            writer.writerow([node])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f37b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges_csv(links, output_file):\n",
    "    # Builds an 'edges' CSV from an adjacency list (directed graph)\n",
    "    \n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"source\", \"target\"])  # We start by writing the headers of our column\n",
    "\n",
    "        for source, targets in links.items():\n",
    "            for target in targets:  \n",
    "                if target != source:  # Avoid self-loops (A -> A)\n",
    "                    writer.writerow([source, target])  # If the target is different form the source (not A -> A), we add the relation in our file\n",
    "                else :\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581d647",
   "metadata": {},
   "source": [
    "CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    {\"name\": \"MBG\",\n",
    "     \"start_url\": \"https://www.mindbodygreen.com/\",\n",
    "     \"crawler_func\": crawl_MBG_seed_links}, # We write the specific function without calling it ()\n",
    "\n",
    "    {\"name\": \"wiki\",\n",
    "     \"start_url\": wiki_seed_pages,\n",
    "     \"crawler_func\": crawl_wiki_seed_links}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d674b1a",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"========================== SOURCES PIPELINE ==========================\"\"\"\n",
    "\n",
    "\"\"\"=== SETUP ===\"\"\"\n",
    "for source in sources:\n",
    "    source_name = source[\"name\"]\n",
    "    start_point = source[\"start_url\"]\n",
    "\n",
    "    files = get_filenames(source_name) # Build all standardized output paths for this source \n",
    "\n",
    "    print(f\"=== SOURCE PROCESSING : {source_name} ===\")\n",
    "    \n",
    "    print(\"\\n=== CRAWLING ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Starting the crawling...{RESET}\")    \n",
    "    crawled_links, dico_links = source[\"crawler_func\"](start_point) # Ex : crawled_links = extract_website_links(https://....)\n",
    "    print(f\"-> Found {len(crawled_links)} links\")\n",
    "\n",
    "    print(\"\\n=== SCRAPING ===\")\n",
    "    \n",
    "    print(f\"{BLUE}[{source_name}] Starting the scraping...{RESET}\")\n",
    "    get_html_content((crawled_links), files['raw'], max=15) # Fetch HTML for each crawled URL and save it as a CSV (url, html)\n",
    "    print(f\"{GREEN}Raw HTML data saved to {files['raw']}{RESET}\")    \n",
    "\n",
    "    print(\"\\n=== CREATE CSV OF NODES ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Creating nodes CSV...{RESET}\")\n",
    "    create_nodes_csv(dico_links, files['nodes']) # We convert the list of links into a dictionary {url: []} to be compatible\n",
    "    print(f\"{GREEN}Nodes CSV saved to {files['nodes']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== CREATE CSV OF EDGES ===\\n\")\n",
    "    print(f\"{BLUE}[{source_name}] Creating edges CSV...{RESET}\")\n",
    "    create_edges_csv(dico_links, files['edges']) # Export directed edges (source -> target) from the adjacency list\n",
    "    print(f\"{GREEN}Edges CSV saved to {files['edges']}{RESET}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links, required_keywords=None, domain=None, already_seen=None):\n",
    "    if required_keywords is None:  # If no list of required keywords is given then we create an empty one\n",
    "        required_keywords = []\n",
    "    if already_seen is None:  # If no list of already seen links is given then we create an empty one\n",
    "        already_seen = set()\n",
    "    \n",
    "    filtered = []\n",
    "    for l in links:\n",
    "        l_lower = l.lower()  # Transformation of capital letter into lower case letter\n",
    "        if required_keywords and not any(keyword.lower() in l_lower for keyword in required_keywords):  # The function skip the url if no required keywords are in the url\n",
    "            continue\n",
    "        if domain and urlparse(l).netloc != domain:  # The function filter the links that are not in the domain\n",
    "            continue\n",
    "        if l in already_seen:  # The function filter the links already in the list of links\n",
    "            continue\n",
    "        filtered.append(l) # If the url past all the filter, it is added to the list of links\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"name\": \"blogs\",\n",
    "     \"start_url\": \"https://bloggers.feedspot.com/lifestyle_blogs/?fbclid=IwY2xjawPEh9BleHRuA2FlbQIxMQBzcnRjBmFwcF9pZAEwAAEeokJD-GCqzqIYQFsjgINZ-moY9eFBlfazeS-PqjVE0WpSysKa_blRr5IkTts_aem_iiBbwTznbHBpSm1tmO_7Eg\",\n",
    "     \"crawler_func\": crawl_feedspot_seed_links,\n",
    "     \"output_file\": \"data/blogs.csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feedspot_seed_pages(url):\n",
    "    \n",
    "    soup = to_soup(url)\n",
    "    links_blogs = set()\n",
    "\n",
    "    if not soup:\n",
    "        print(f\"{RED}Error: Could not fetch main blogs page: {url}{RESET}\")\n",
    "        return set()\n",
    "\n",
    "    blogs = soup.find_all('a', href=True) # and tag.get('class') and 'wb-ba' in tag.get('class') and any('ext' in c for c in tag.get('class')))\n",
    "\n",
    "    for link in blogs:\n",
    "        href = link.get('href') #if link.name == 'a' else link.text.strip()  if href and \"http\" in href and \"bloggers.feedspot.com\" not in href:\n",
    "        \n",
    "        if not href.startswith('https?'):\n",
    "            href = urljoin(url, href)\n",
    "\n",
    "        if urlparse(href).netloc != urlparse(url).netloc:\n",
    "            if any(filter in href for filter in filters_noise):\n",
    "                continue\n",
    "            if \"mindbodygreen.com\" in href:\n",
    "                 continue\n",
    "            if href and \"http\" in href and \"feedspot\" not in href :\n",
    "                links_blogs.add(href)\n",
    "    \n",
    "    return links_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1963b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exemple d'utilisation ===\n",
    "url_test = \"https://bloggers.feedspot.com/lifestyle_blogs/?fbclid=IwY2xjawPEh9BleHRuA2FlbQIxMQBzcnRjBmFwcF9pZAEwAAEeokJD-GCqzqIYQFsjgINZ-moY9eFBlfazeS-PqjVE0WpSysKa_blRr5IkTts_aem_iiBbwTznbHBpSm1tmO_7Eg\"\n",
    "\n",
    "crawl = extract_feedspot_seed_pages(url_test)\n",
    "print(len(crawl))\n",
    "print(crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exemple d'utilisation ===\n",
    "url_test = list(crawl)[:2]\n",
    "\n",
    "for url in url_test:\n",
    "\n",
    "    # Crawl en largeur\n",
    "    breath_links, last_visited = extract_links_breadth_first(url, max_levels=1)\n",
    "    print(\"Breath Links:\", len(breath_links))\n",
    "    print(breath_links)\n",
    "    print(\"Last Level Links:\", len(last_visited))\n",
    "    print(last_visited)\n",
    "\n",
    "    # Crawl en profondeur\n",
    "    depth_links = extract_links_depth_from_list(\n",
    "        start_links=last_visited, visited_links=breath_links.copy(),  # on copie pour ne pas modifier l'original\n",
    "        max_depth=1, max_links_per_page=1)\n",
    "    print(\"Depth Links:\", len(depth_links))\n",
    "    print(depth_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
