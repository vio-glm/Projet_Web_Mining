{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a96ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re \n",
    "import token\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c7a5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF COLORS ---\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8001b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\"}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bda027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters_MBG = [r'/products?/', r'/shop/', r'/search?', r'/store/', r'/cart', r'/checkout', r'/account', \n",
    "           r'/login', r'/register', r'/tag', r'/search', r'#', \"youtube.com\", \n",
    "           \"instagram.com\", \"facebook.com\", \"twitter.com\", \"pinterest.com\", \"linkedin.com\", \"tiktok.com\", \n",
    "           \"amazon.com\", \"google.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9667e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_verify_url(url) :\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code != 200:  # If the status code is not OK (200) the function return none and an error message\n",
    "            print(f\"Failed to fetch the url: {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        return response  \n",
    "    except requests.RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "622f3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_soup(url):\n",
    "    response = fetch_verify_url(url)\n",
    "    if response:  # If the response is not none, the function return the beautiful soup object\n",
    "        return BeautifulSoup(response.text, 'html.parser') \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_MBG_links(url_MBG, max_levels=3):\n",
    "    queue = deque([(url_MBG, 0)])\n",
    "    links_MBG = set([url_MBG])\n",
    "    start_domain = urlparse(url_MBG).netloc\n",
    "\n",
    "    while queue:\n",
    "        current_url, current_level = queue.popleft()\n",
    "\n",
    "        # Si on atteint le niveau max, on arrête de creuser à partir d'ici\n",
    "        if current_level >= max_levels:\n",
    "            continue\n",
    "    \n",
    "        soup = to_soup(current_url)\n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        content_area = soup.find('div', id='content') or soup.body\n",
    "        if not content_area:\n",
    "            continue\n",
    "\n",
    "        for item in content_area.find_all('a', href=True):\n",
    "            href = item.get('href')\n",
    "            full_url = urljoin(current_url, href)\n",
    "                    \n",
    "            # Si nouveau lien trouvé\n",
    "            if any(filter in full_url for filter in Filters_MBG):\n",
    "                continue\n",
    "            \n",
    "            if full_url not in links_MBG:\n",
    "                links_MBG.add(full_url)\n",
    "\n",
    "                # On ajoute à la queue SEULEMENT si c'est interne (pour continuer le crawl)\n",
    "                if urlparse(full_url).netloc == start_domain:\n",
    "                    queue.append((full_url, current_level + 1))\n",
    "                    \n",
    "    return links_MBG\n",
    "\n",
    "ALL_MBG_LINKS = extract_MBG_links(\"https://www.mindbodygreen.com/\")\n",
    "print(ALL_MBG_LINKS)\n",
    "print(len(ALL_MBG_LINKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73087380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filepath():\n",
    "\n",
    "    user_filename = input(\"Enter the name of the file to save (e.g. : test) : \").strip() #strip deletes spaces before and after the string\n",
    "    \n",
    "    if not user_filename : \n",
    "        user_filename = \"test\"\n",
    "    \n",
    "    if not user_filename.endswith('.csv'): # Verify if the filename ends with .csv\n",
    "        user_filename += '.csv' # Append .csv if not present\n",
    "    filename = os.path.join(\"data\", user_filename) # Construct the full file path\n",
    "\n",
    "    # Create directory if not exists\n",
    "    directory = os.path.dirname(filename)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"{GREEN}Created directory : {os.path.abspath(directory)}{RESET}\") # Full path info\n",
    "    print(f'{BLUE}Output will be saved to : \"{filename}\"{RESET}')\n",
    "\n",
    "    # Avertissement si écrasement (juste pour info)\n",
    "    if os.path.exists(filename):\n",
    "        print(\"{BLUE}Note : The file is overwritten{RESET}\")\n",
    "    \n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(links):\n",
    "    content = []\n",
    "    failed_links = []\n",
    "    #filename = create_filepath()\n",
    "\n",
    "    for link in links:\n",
    "        response = fetch_verify_url(link)\n",
    "        if response:\n",
    "            content.append({'url': link, 'html': response.text})\n",
    "        else:\n",
    "            failed_links.append(link)\n",
    "\n",
    "    print(f\"{GREEN}Fetched: {len(content)} {BLUE}|{RED} Failed: {len(failed_links)}{RESET}\")\n",
    "\n",
    "    return content, failed_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    if not data:\n",
    "        print(f\"{RED}No data to save.{RESET}\")\n",
    "        return\n",
    "\n",
    "    # Get the keys from the first dictionary for the CSV header from get_html_content\n",
    "    fieldnames = data[0].keys() # Detection of the existing colons in the data file\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f: # Opens the csv file as utf-8\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL) # Initialising a writer to write the dictionary into the csv file\n",
    "            writer.writeheader() # writes the colons headers\n",
    "            writer.writerows(data) # writes the rows\n",
    "        print(f\"{GREEN}Success! Data saved to: {os.path.abspath(filename)}{RESET}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{RED}Error saving file: {e}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_files() :\n",
    "    print(\"Files in data directory :\")\n",
    "    print(os.listdir(\"data\"))\n",
    "\n",
    "print(get_name_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(html):\n",
    "    if not html: \n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    raw_text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    text = ' '.join(raw_text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'html' not in df.columns:  # Verify that the html colon exists\n",
    "        raise ValueError(f\"The html column is missing in: {input_csv}\")\n",
    "\n",
    "    df['corpus_text'] = df['html'].apply(get_corpus)  # Cleans the html column\n",
    "    df = df[['url', 'corpus_text']]  # Keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # Creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SCRAP URL : https://www.mindbodygreen.com/\"\"\"\n",
    "\n",
    "# 1. Create the filename (\"FINAL_MBG\")\n",
    "target_filename = create_filepath()\n",
    "\n",
    "# 2. Extract the data\n",
    "content, failed_links, filename = get_html_content(list(ALL_MBG_LINKS)) #(I did it in a previous cell)\n",
    "\n",
    "# 3. Save the data to the file\n",
    "save_to_csv(content, target_filename)\n",
    "df = pd.read_csv(target_filename)  # df is now a table with the data from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844df3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_html(text):\n",
    "    text = text.lower()  # convert all letters to lowercase\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers like [1], [2], etc.\n",
    "    text = re.sub(r'\\d+', ' ', text)  # remove all numbers\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # keep only English letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with a single space\n",
    "    return text.strip()  # remove leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the cleaned text as the input\n",
    "\n",
    "    if 'cleaned_text' not in df.columns:  # Verify that the cleaned text colon exists\n",
    "        raise ValueError(f\"The cleaned text is missing in: {input_csv}\")\n",
    "\n",
    "    df['normalized_text'] = df['cleaned_text'].apply(normalize_html)  # normalize the cleaned text\n",
    "    df = df[['url', 'normalized_text']]  # keep the url and normalized text \n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # creats a new csv file as the output of the function\n",
    "\n",
    "    return df\n",
    "\n",
    "cleaned_MBG_csv = \"data/mbg_cleaned_pages.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12db6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english'))) + [\"'s\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#stem = nltk.stem.SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_html(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]  # remove punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # remove stopwords\n",
    "    #tokens = [stem.stem(token) for token in tokens]  # apply stemming (racinisation)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # apply lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the normalized text as the input\n",
    "\n",
    "    if 'normalized_text' not in df.columns:  # Verify that the normalized text colon exists\n",
    "        raise ValueError(f\"The normalized text is missing in: {input_csv}\")\n",
    "\n",
    "    df['tokenized_text'] = df['normalized_text'].apply(tokenize_html)  # cleans the html colon\n",
    "    df = df[['url', 'tokenized_text']]  # keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd53c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#content, failed, raw_filename = get_html_content(list(ALL_MBG_LINKS), max=10) déjà run \n",
    "\n",
    "if raw_filename: \n",
    "    print(f\"Fichier brut généré : {raw_filename}\")\n",
    "    \n",
    "    # 2. GENERATION DU CORPUS\n",
    "    # On déduit le nom du fichier de sortie (ex: 'data/mbg_raw_corpus.csv')\n",
    "    corpus_filename = raw_filename.replace(\".csv\", \"_corpus.csv\")\n",
    "    \n",
    "    print(f\"Traitement du corpus vers -> {corpus_filename} ...\")\n",
    "    \n",
    "    # Appel de ta fonction avec le nom qu'on vient de récupérer\n",
    "    df_corpus = corpus_csv_file(raw_filename, corpus_filename)\n",
    "    \n",
    "    print(\"✅ Corpus généré avec succès !\")\n",
    "    print(df_corpus.head())\n",
    "else:\n",
    "    print(\"❌ Pas de fichier brut, impossible de faire le corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc42af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wiki_csv = corpus_csv_file(\"wikipedia_lifestyle_content.csv\", \"wikipedia_lifestyle_corpus.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
