{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648a76d6",
   "metadata": {},
   "source": [
    "IMPORT IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "dd99e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, deque\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re \n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import time\n",
    "import token \n",
    "from urllib.parse import urljoin, urlparse, unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "540f5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Nécessaire pour les nouvelles versions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')   # Souvent requis par le lemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ac71",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5c7a5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF COLORS ---\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8001b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF HEADERS ---\n",
    "\n",
    "HEADER_BROWSER = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\"}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADER_BROWSER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fb002d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF FILTERS ---\n",
    "\n",
    "filters_noise = [r'/products?/', r'/shop/', r'/store/', r'/cart', r'/checkout', r'/account', \n",
    "           r'/login', r'/register', r'/tag', r'/search', r'/about', r'contact', r'privacy', r'terms', r'#', \"youtube.com\", \n",
    "           \"instagram.com\", \"facebook.com\", \"twitter.com\", \"pinterest.com\", \"linkedin.com\", \"tiktok.com\", \n",
    "           \"amazon.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fe11fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION OF SEED LINKS FOR WIKIPEDIA ---\n",
    "\n",
    "wiki_seed_pages = [ # Our selection of a few pages = starting list\n",
    "    \"https://en.wikipedia.org/wiki/Lifestyle\",\n",
    "    \"https://en.wikipedia.org/wiki/Quality_of_life\",\n",
    "    \"https://en.wikipedia.org/wiki/Standard_of_living\",\n",
    "    \"https://en.wikipedia.org/wiki/Healthy_lifestyle\",\n",
    "    \"https://en.wikipedia.org/wiki/Physical_fitness\",\n",
    "    \"https://en.wikipedia.org/wiki/Well-being\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Healthy_diet\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Work%E2%80%93life_balance\", \n",
    "    \"https://en.wikipedia.org/wiki/Leisure\",\n",
    "    \"https://en.wikipedia.org/wiki/Hobby\",\n",
    "    \"https://en.wikipedia.org/wiki/Travel\",\n",
    "    \"https://en.wikipedia.org/wiki/Outdoor_recreation\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_living\",\n",
    "    \"https://en.wikipedia.org/wiki/Sport\",\n",
    "    \"https://en.wikipedia.org/wiki/Home\",\n",
    "    \"https://en.wikipedia.org/wiki/Fashion\",\n",
    "    \"https://en.wikipedia.org/wiki/Personal_care\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1637a4",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a37e18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(source_name):\n",
    "    directory_name = \"data\"\n",
    "    if not os.path.exists(directory_name):\n",
    "            os.makedirs(directory_name)\n",
    "            print(f\"{GREEN}Created directory : {directory_name}{RESET}\")\n",
    "    \n",
    "    # Avertissement si écrasement (juste pour info)\n",
    "    if os.path.exists(source_name):\n",
    "        print(f\"{BLUE}Note : The file is overwritten{RESET}\") \n",
    "    \n",
    "    base_name = source_name.replace(\" \", \"_\")\n",
    "\n",
    "    return {\"raw\":       os.path.join(directory_name, f\"{base_name}_raw_html.csv\"),\n",
    "            \"corpus\":    os.path.join(directory_name, f\"{base_name}_corpus.csv\"),\n",
    "            \"cleaned\":   os.path.join(directory_name, f\"{base_name}_cleaned.csv\"),\n",
    "            \"norm\":      os.path.join(directory_name, f\"{base_name}_corpus_norm.csv\"),\n",
    "            \"tokenized\": os.path.join(directory_name, f\"{base_name}_corpus_tokenized.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f52e9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url) :\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code == 200: \n",
    "            return response # If the status code is not OK (200) the function return none and an error message\n",
    "        else:\n",
    "            print(f\"Failed to fetch the url: {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    except requests.RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "98a2c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_soup(url):\n",
    "    response = fetch_url(url)\n",
    "    if response:  # If the response is not none, the function return the beautiful soup object\n",
    "        return BeautifulSoup(response.text, 'html.parser') \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b9bd29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_website_links(url, max_levels=3): \n",
    "    queue = deque([(url, 0)])\n",
    "    visited_links = set([url])\n",
    "    start_domain = urlparse(url).netloc\n",
    "\n",
    "    is_feedspot = \"feedspot.com\" in start_domain\n",
    "\n",
    "    while queue:\n",
    "        current_url, current_level = queue.popleft()\n",
    "\n",
    "        if current_level >= max_levels:\n",
    "            continue\n",
    "    \n",
    "        soup = to_soup(current_url)\n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        content_area = soup.body # Stay large (here) to get more links\n",
    "        if not content_area:\n",
    "            continue\n",
    "\n",
    "        for item in content_area.find_all('a', href=True):\n",
    "            href = item.get('href')\n",
    "            if href.startswith('mailto:') or href.startswith('tel:') or href.startswith('javascript:'):\n",
    "                continue\n",
    "\n",
    "            full_url = urljoin(current_url, href)\n",
    "            full_url = full_url.split('?')[0].split('#')[0]\n",
    "            full_url = full_url.lower()\n",
    "\n",
    "            # Si nouveau lien trouvé\n",
    "            for filter in filters_noise:\n",
    "                if filter in full_url:\n",
    "                    break\n",
    "            else:\n",
    "                if full_url not in visited_links:\n",
    "                    parsed_url = urlparse(full_url)\n",
    "                    if is_feedspot: # Sur feedspot on prend uniquement les liens sortants\n",
    "                        if parsed_url.netloc != start_domain:\n",
    "                            visited_links.add(full_url)\n",
    "                    else:\n",
    "                        if parsed_url.netloc == start_domain: # Sur MBG on crawl uniquement les liens qui ont le même domaine\n",
    "                            visited_links.add(full_url)\n",
    "                            queue.append((full_url, current_level + 1))\n",
    "                    \n",
    "    return visited_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d4380657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_links(url):\n",
    "    soup = to_soup(url)\n",
    "\n",
    "    if not soup:\n",
    "        return set()\n",
    "\n",
    "    wiki_links = set()\n",
    "\n",
    "    # --- Main content ---\n",
    "    to_visit = soup.find_all('div', class_=\"mw-content-ltr mw-parser-output\") # Specific to wikipedia\n",
    "\n",
    "    for div in to_visit:\n",
    "        for item in div.find_all('a', href=True):\n",
    "            href = item.get('href')\n",
    "            if href.startswith(\"/wiki/\") and \":\" not in href:\n",
    "                wiki_links.add(urljoin(url, href))\n",
    "\n",
    "    return wiki_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b032dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feedspot_seed_pages(url):\n",
    "    \n",
    "    soup = to_soup(url)\n",
    "    links_blogs = set()\n",
    "\n",
    "    if not soup:\n",
    "        print(f\"{RED}Error: Could not fetch main blogs page: {url}{RESET}\")\n",
    "        return set()\n",
    "\n",
    "    blogs = soup.find_all('a', href=True) # and tag.get('class') and 'wb-ba' in tag.get('class') and any('ext' in c for c in tag.get('class')))\n",
    "\n",
    "    for link in blogs:\n",
    "        href = link.get('href') #if link.name == 'a' else link.text.strip()\n",
    "        \n",
    "        if not href.startswith('https?'):\n",
    "            href = urljoin(url, href)\n",
    "\n",
    "        if urlparse(href).netloc != urlparse(url).netloc:\n",
    "            if any(filter in href for filter in filters_noise):\n",
    "                continue\n",
    "            if \"mindbodygreen.com\" in href:\n",
    "                 continue\n",
    "            if href not in links_blogs:\n",
    "                links_blogs.add(href)\n",
    "    \n",
    "    return links_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fdd31400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wiki_seed_links(wiki_seed_pages):\n",
    "    wiki_all_links = set(wiki_seed_pages) # Set() avoids duplication, unlike lists\n",
    "    dico_wiki_links = {}\n",
    "\n",
    "    for url in wiki_seed_pages:\n",
    "        crawl_links = extract_wikipedia_links(url)\n",
    "        dico_wiki_links[url] = crawl_links\n",
    "        wiki_all_links.update(crawl_links) # Only add new links\n",
    "\n",
    "    return wiki_all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "325bec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_feedspot_seed_links(feedspot_seed_page):\n",
    "    feedspot_all_links = set()\n",
    "    dico_blogs_links = {}\n",
    "    excluded = [\"mindbodygreen.com\", \"www.mindbodygreen.com\"]\n",
    "\n",
    "    blogs_list = extract_feedspot_seed_pages(feedspot_seed_page)\n",
    "    print(f\" -> {len(blogs_list)} blogs trouvés sur Feedspot.\")\n",
    "\n",
    "    for url in blogs_list:\n",
    "        if any(ex in url for ex in excluded):\n",
    "            continue\n",
    "        \n",
    "        crawl_links = extract_website_links(url, max_levels=1)\n",
    "        dico_blogs_links[url] = crawl_links\n",
    "        feedspot_all_links.update(crawl_links)\n",
    "    \n",
    "    return feedspot_all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4b81deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_url_to_label(url):\n",
    "    title = urlparse(url).path.split(\"/\")[-1]\n",
    "    title = unquote(title)\n",
    "    title = title.replace(\"_\", \" \")\n",
    "    # ajout d'un suffixe si la page contient parenthèses\n",
    "    if \"(\" in title:\n",
    "        title = title.replace(\"(\", \" (\").replace(\")\", \")\")\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "27f13838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"{RED}No data to save.{RESET}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"{RED} ERROR : File {filename} was not created\")\n",
    " \n",
    "    # Get the keys from the first dictionary for the CSV header from get_html_content\n",
    "    fieldnames = data[0].keys() # Detection of the existing colons in the data file\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f: # Opens the csv file as utf-8\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL) # Initialising a writer to write the dictionary into the csv file\n",
    "            writer.writeheader() # writes the colons headers\n",
    "            writer.writerows(data) # writes the rows\n",
    "        print(f\"{GREEN}Success! Data saved to: {RESET}{os.path.abspath(filename)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{RED}Error saving file: {e}{RESET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3834f",
   "metadata": {},
   "source": [
    "retirer le max=None et links[:max] pour run tout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "92cd0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(links, output_csv, max=None):\n",
    "    content = []\n",
    "    failed_links = []\n",
    "\n",
    "    for link in links[:max]:\n",
    "        response = fetch_url(link)\n",
    "        if response:\n",
    "            content.append({'url': link, 'html': response.text})\n",
    "        else:\n",
    "            failed_links.append(link)\n",
    "\n",
    "    print(f\"{GREEN}Fetched: {len(content)}{RESET}\")\n",
    "    print(f\"{RED}Failed: {len(failed_links)}{RESET}\")\n",
    "\n",
    "    if output_csv and content:\n",
    "        save_to_csv(content, output_csv)\n",
    "\n",
    "    return content, failed_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "eec302d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "\n",
    "    if html is None or pd.isna(html) or not isinstance(html, str):\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    for tag in soup(['script', 'style', 'noscript']):  # Supress any unessecary tags\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)  # Collect all visible text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supress any unecessary spaces\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "cfd7ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the raw html as the input\n",
    "\n",
    "    if 'corpus_text' not in df.columns:  # Verify that the html colon exists\n",
    "        raise ValueError(f\"The corpus column is missing in: {input_csv}\")\n",
    "\n",
    "    df['corpus_text'] = df['corpus_text'].fillna(\"\")   \n",
    "    df['cleaned_text'] = df['corpus_text'].apply(clean_html)  # Cleans the html column\n",
    "    df = df[['url', 'cleaned_text']]  # Keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # Creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "104b6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(html):\n",
    "    if not html: \n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\") #pip install lmxl requis\n",
    "    except:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    unwanted_tags = [\"script\", \"style\", \"nav\", \"noscript\", \"header\", \"footer\", \"aside\", \"form\", \"input\", \"button\", \"meta\", \"svg\"]\n",
    "\n",
    "    for tag in soup(unwanted_tags):\n",
    "        tag.decompose()\n",
    "    \n",
    "    is_wikipedia = soup.find_all('div', class_=\"mw-content-ltr mw-parser-output\")\n",
    "\n",
    "    if not is_wikipedia: # For blogs\n",
    "        regex_string = r\"cookie-banner|popup|ad(vertisement)?|social|share|banner|newsletter|widget\"\n",
    "        unwanted_patterns = re.compile(regex_string, re.IGNORECASE)\n",
    "\n",
    "        tags_to_remove = soup.find_all(attrs={\"class\": unwanted_patterns}) + soup.find_all(attrs={\"id\": unwanted_patterns})     \n",
    "\n",
    "        for tag in tags_to_remove:\n",
    "            tag.decompose()\n",
    "        \n",
    "    raw_text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(raw_text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "57c811b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df['corpus_text'] = df['html'].apply(get_corpus)\n",
    "    df[['url', 'corpus_text']].to_csv(output_csv, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e007b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_html(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()  # convert all letters to lowercase\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers like [1], [2], etc.\n",
    "    text = re.sub(r'\\d+', ' ', text)  # remove all numbers\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # keep only English letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with a single space\n",
    "    return text.strip()  # remove leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fed894ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the cleaned text as the input\n",
    "\n",
    "    if 'cleaned_text' not in df.columns:  # Verify that the cleaned text colon exists\n",
    "        raise ValueError(f\"The cleaned text is missing in: {input_csv}\")\n",
    "\n",
    "    df['cleaned_text'] = df['cleaned_text'].fillna(\"\") # On force tout en string vide si c'est NaN\n",
    "    df['normalized_text'] = df['cleaned_text'].apply(normalize_html)  # normalize the cleaned text\n",
    "    df = df[['url', 'normalized_text']]  # keep the url and normalized text \n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # creats a new csv file as the output of the function\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12db6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') \n",
    "stop_words = list(set(stopwords.words('english'))) + [\"'s\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#stem = nltk.stem.SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_html(text):\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return [] # Return empty list for non-string or NaN inputs (= when text is missing)\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]  # remove punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # remove stopwords\n",
    "    #tokens = [stem.stem(token) for token in tokens]  # apply stemming (racinisation)\n",
    "    tokens = [token for token in tokens if len(token) > 1]  # Remove very short or meaningless tokens \n",
    "    tokens = [token for token in tokens if len(token) < 20]  # Remove very long or meaningless tokens \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # apply lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8771b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_csv_file(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)  # Take the csv file with the normalized text as the input\n",
    "\n",
    "    if 'normalized_text' not in df.columns:  # Verify that the normalized text colon exists\n",
    "        raise ValueError(f\"The normalized text is missing in: {input_csv}\")\n",
    "\n",
    "    df['normalized_text'] = df['normalized_text'].fillna(\"\") # Replace holes by nothing to avoid error due to 'NaN'\n",
    "\n",
    "    df['tokenized_text'] = df['normalized_text'].apply(tokenize_html)  # cleans the html colon\n",
    "    df = df[['url', 'tokenized_text']]  # keep the url and text colon (not the raw html)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  # creats a new csv file as the output of the function\n",
    "    print(df.head()) # Display the 5 first lines of the final dataframe\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581d647",
   "metadata": {},
   "source": [
    "CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cc9d542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    {\"name\": \"MBG\",\n",
    "     \"start_url\": \"https://www.mindbodygreen.com/\",\n",
    "     \"crawler_func\": extract_website_links,\n",
    "     \"output_file\": \"data/MBG.csv\"},  # We pass the specific function without calling it ()\n",
    "    {\"name\": \"wiki\",\n",
    "     \"start_url\": wiki_seed_pages,\n",
    "     \"crawler_func\": crawl_wiki_seed_links,\n",
    "     \"output_file\": \"data/wiki.csv\"},\n",
    "    {\"name\": \"blogs\",\n",
    "     \"start_url\": \"https://bloggers.feedspot.com/lifestyle_blogs/?fbclid=IwY2xjawPEh9BleHRuA2FlbQIxMQBzcnRjBmFwcF9pZAEwAAEeokJD-GCqzqIYQFsjgINZ-moY9eFBlfazeS-PqjVE0WpSysKa_blRr5IkTts_aem_iiBbwTznbHBpSm1tmO_7Eg\",\n",
    "     \"crawler_func\": crawl_feedspot_seed_links,\n",
    "     \"output_file\": \"data/blogs.csv\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d674b1a",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f9fdb046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SOURCE TRAITEMENT : MBG ===\n",
      "\n",
      "=== CRAWLING ===\n",
      "\u001b[94m[MBG] Starting the crawling...\u001b[0m\n",
      "-> Found 4331 links\n",
      "\n",
      "=== SCRAPING ===\n",
      "\u001b[94m[MBG] Starting the scraping...\u001b[0m\n",
      "\u001b[92mFetched: 10\u001b[0m\n",
      "\u001b[91mFailed: 0\u001b[0m\n",
      "\u001b[92mSuccess! Data saved to: \u001b[0mc:\\Users\\User\\Desktop\\Web Mining\\Projet 2025 - Arshik, Violaine\\Projet_Web_Mining\\Data_Collection_Jupyter_Lab\\data\\MBG_raw_html.csv\n",
      "\u001b[92mRaw HTML data saved to data\\MBG_raw_html.csv\u001b[0m\n",
      "\n",
      "=== CORPUS EXTRACTION ===\n",
      "\u001b[94m[MBG] Extracting the corpus...\u001b[0m\n",
      "\u001b[92mCorpus data saved to data\\MBG_corpus.csv\u001b[0m\n",
      "\n",
      "=== CLEANING ===\n",
      "\u001b[94m[MBG] Cleaning the data...\u001b[0m\n",
      "\u001b[92mCleaned text saved to data\\MBG_cleaned.csv\u001b[0m\n",
      "\n",
      "=== NORMALIZATION ===\n",
      "\u001b[94m[MBG] Normalizing the data...\u001b[0m\n",
      "\u001b[92mNormalized text saved to data\\MBG_corpus_norm.csv\u001b[0m\n",
      "\n",
      "=== TOKENIZATION ===\n",
      "\u001b[94m[MBG] Tokenizing text...\u001b[0m\n",
      "                                                 url  \\\n",
      "0  https://www.mindbodygreen.com/articles/solawav...   \n",
      "1  https://www.mindbodygreen.com/articles/read-th...   \n",
      "2  https://www.mindbodygreen.com/articles/keep-cl...   \n",
      "3  https://www.mindbodygreen.com/articles/scienti...   \n",
      "4  https://www.mindbodygreen.com/articles/5-tips-...   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [solawave, wrinkle, retreat, light, therapy, m...  \n",
      "1  [ok, want, kid, convince, reason, mindbodygree...  \n",
      "2  [important, part, home, keep, clean, feng, shu...  \n",
      "3  [process, food, addictive, eat, less, mindbody...  \n",
      "4  [tip, create, even, routine, great, sleep, min...  \n",
      "\u001b[92mTokenized data in data\\MBG_corpus_tokenized.csv\u001b[0m\n",
      "\n",
      "=== SOURCE TRAITEMENT : wiki ===\n",
      "\n",
      "=== CRAWLING ===\n",
      "\u001b[94m[wiki] Starting the crawling...\u001b[0m\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Hobby with status code 403\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Sustainable_living with status code 403\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Fashion with status code 403\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Personal_care with status code 403\n",
      "-> Found 5379 links\n",
      "\n",
      "=== SCRAPING ===\n",
      "\u001b[94m[wiki] Starting the scraping...\u001b[0m\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Individual_sport with status code 403\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Telepsychiatry with status code 403\n",
      "Failed to fetch the url: https://en.wikipedia.org/wiki/Thermoregulation_in_humans with status code 403\n",
      "\u001b[92mFetched: 7\u001b[0m\n",
      "\u001b[91mFailed: 3\u001b[0m\n",
      "\u001b[92mSuccess! Data saved to: \u001b[0mc:\\Users\\User\\Desktop\\Web Mining\\Projet 2025 - Arshik, Violaine\\Projet_Web_Mining\\Data_Collection_Jupyter_Lab\\data\\wiki_raw_html.csv\n",
      "\u001b[92mRaw HTML data saved to data\\wiki_raw_html.csv\u001b[0m\n",
      "\n",
      "=== CORPUS EXTRACTION ===\n",
      "\u001b[94m[wiki] Extracting the corpus...\u001b[0m\n",
      "\u001b[92mCorpus data saved to data\\wiki_corpus.csv\u001b[0m\n",
      "\n",
      "=== CLEANING ===\n",
      "\u001b[94m[wiki] Cleaning the data...\u001b[0m\n",
      "\u001b[92mCleaned text saved to data\\wiki_cleaned.csv\u001b[0m\n",
      "\n",
      "=== NORMALIZATION ===\n",
      "\u001b[94m[wiki] Normalizing the data...\u001b[0m\n",
      "\u001b[92mNormalized text saved to data\\wiki_corpus_norm.csv\u001b[0m\n",
      "\n",
      "=== TOKENIZATION ===\n",
      "\u001b[94m[wiki] Tokenizing text...\u001b[0m\n",
      "                                               url  \\\n",
      "0  https://en.wikipedia.org/wiki/LCCN_(identifier)   \n",
      "1    https://en.wikipedia.org/wiki/Maternal_health   \n",
      "2       https://en.wikipedia.org/wiki/Orienteering   \n",
      "3          https://en.wikipedia.org/wiki/Pessimism   \n",
      "4           https://en.wikipedia.org/wiki/Holocene   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [library, congress, control, number, wikipedia...  \n",
      "1  [maternal, health, wikipedia, jump, content, w...  \n",
      "2  [orienteering, wikipedia, jump, content, wikip...  \n",
      "3  [pessimism, wikipedia, jump, content, wikipedi...  \n",
      "4  [holocene, wikipedia, jump, content, wikipedia...  \n",
      "\u001b[92mTokenized data in data\\wiki_corpus_tokenized.csv\u001b[0m\n",
      "\n",
      "=== SOURCE TRAITEMENT : blogs ===\n",
      "\n",
      "=== CRAWLING ===\n",
      "\u001b[94m[blogs] Starting the crawling...\u001b[0m\n",
      " -> 132 blogs trouvés sur Feedspot.\n",
      "Failed to fetch the url: https://goop.com/ with status code 403\n",
      "Failed to fetch the url: https://amelialiana.com/ with status code 403\n",
      "Failed to fetch the url: https://livingswag.com/ with status code 500\n",
      "-> Found 5134 links\n",
      "\n",
      "=== SCRAPING ===\n",
      "\u001b[94m[blogs] Starting the scraping...\u001b[0m\n",
      "\u001b[92mFetched: 10\u001b[0m\n",
      "\u001b[91mFailed: 0\u001b[0m\n",
      "\u001b[92mSuccess! Data saved to: \u001b[0mc:\\Users\\User\\Desktop\\Web Mining\\Projet 2025 - Arshik, Violaine\\Projet_Web_Mining\\Data_Collection_Jupyter_Lab\\data\\blogs_raw_html.csv\n",
      "\u001b[92mRaw HTML data saved to data\\blogs_raw_html.csv\u001b[0m\n",
      "\n",
      "=== CORPUS EXTRACTION ===\n",
      "\u001b[94m[blogs] Extracting the corpus...\u001b[0m\n",
      "\u001b[92mCorpus data saved to data\\blogs_corpus.csv\u001b[0m\n",
      "\n",
      "=== CLEANING ===\n",
      "\u001b[94m[blogs] Cleaning the data...\u001b[0m\n",
      "\u001b[92mCleaned text saved to data\\blogs_cleaned.csv\u001b[0m\n",
      "\n",
      "=== NORMALIZATION ===\n",
      "\u001b[94m[blogs] Normalizing the data...\u001b[0m\n",
      "\u001b[92mNormalized text saved to data\\blogs_corpus_norm.csv\u001b[0m\n",
      "\n",
      "=== TOKENIZATION ===\n",
      "\u001b[94m[blogs] Tokenizing text...\u001b[0m\n",
      "                                                 url  \\\n",
      "0          https://www.primermagazine.com/gift-ideas   \n",
      "1  https://thestripe.com/great-books-to-read-on-a...   \n",
      "2  https://podcast.feedspot.com/emma_grede_podcasts/   \n",
      "3  https://alysonhaley.com/2025/03/exclusive-disc...   \n",
      "4         http://hollybeetells.com/money-and-saving/   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0                              [primer, gift, guide]  \n",
      "1  [stripe, favorite, thing, vacation, specifical...  \n",
      "2  [best, ina, garten, podcast, look, emma, grede...  \n",
      "3                                                 []  \n",
      "4  [money, save, archive, hbt, link, navigate, pa...  \n",
      "\u001b[92mTokenized data in data\\blogs_corpus_tokenized.csv\u001b[0m\n",
      "\n",
      "\n",
      "=== MERGING DATASETS ===\n",
      "\u001b[92mSUCCESS ! Final data in GLOBAL_dataset_tokenized.csv\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"========================== SOURCES PIPELINE ==========================\"\"\"\n",
    "\n",
    "\"\"\"=== SETUP ===\"\"\"\n",
    "for source in sources:\n",
    "    filename = source[\"output_file\"]\n",
    "    source_name = source[\"name\"]\n",
    "    start_point = source[\"start_url\"]\n",
    "\n",
    "    files = get_filenames(source_name)\n",
    "\n",
    "    print(f\"=== SOURCE TRAITEMENT : {source_name} ===\")\n",
    "\n",
    "    print(\"\\n=== CRAWLING ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Starting the crawling...{RESET}\")    \n",
    "    crawled_links = source[\"crawler_func\"](start_point) # Ex : crawled_links = extract_website_links(https://....)\n",
    "    print(f\"-> Found {len(crawled_links)} links\")\n",
    "\n",
    "    print(\"\\n=== SCRAPING ===\")\n",
    "    \n",
    "    print(f\"{BLUE}[{source_name}] Starting the scraping...{RESET}\")\n",
    "    get_html_content(list(crawled_links), files['raw'], max=10)\n",
    "    print(f\"{GREEN}Raw HTML data saved to {files['raw']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== CORPUS EXTRACTION ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Extracting the corpus...{RESET}\")\n",
    "    extract_corpus_csv_file(files[\"raw\"], files[\"corpus\"])\n",
    "    print(f\"{GREEN}Corpus data saved to {files['corpus']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== CLEANING ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Cleaning the data...{RESET}\")\n",
    "    clean_csv_file(files[\"corpus\"], files[\"cleaned\"])\n",
    "    print(f\"{GREEN}Cleaned text saved to {files['cleaned']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== NORMALIZATION ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Normalizing the data...{RESET}\")\n",
    "    normalize_csv_file(files[\"cleaned\"], files[\"norm\"])\n",
    "    print(f\"{GREEN}Normalized text saved to {files['norm']}{RESET}\")\n",
    "\n",
    "    print(\"\\n=== TOKENIZATION ===\")\n",
    "\n",
    "    print(f\"{BLUE}[{source_name}] Tokenizing text...{RESET}\")\n",
    "    tokenize_csv_file(files[\"norm\"], files[\"tokenized\"])\n",
    "    print(f\"{GREEN}Tokenized data in {files['tokenized']}{RESET}\\n\")\n",
    "\n",
    "print(f\"\\n=== MERGING DATASETS ===\")\n",
    "all_dfs = []\n",
    "for source in sources:\n",
    "    f_token = get_filenames(source[\"name\"])[\"tokenized\"]\n",
    "    df = pd.read_csv(f_token)\n",
    "    df['source_origin'] = source[\"name\"] # On garde une trace de l'origine !\n",
    "    all_dfs.append(df)\n",
    "\n",
    "df_global = pd.concat(all_dfs, ignore_index=True)\n",
    "df_global.to_csv(\"GLOBAL_dataset_tokenized.csv\", index=False)\n",
    "print(f\"{GREEN}SUCCESS ! Final data in GLOBAL_dataset_tokenized.csv{RESET}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9fa0c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links, required_keywords=None, domain=None, already_seen=None):\n",
    "    if required_keywords is None:  # If no list of required keywords is given then we create an empty one\n",
    "        required_keywords = []\n",
    "    if already_seen is None:  # If no list of already seen links is given then we create an empty one\n",
    "        already_seen = set()\n",
    "    \n",
    "    filtered = []\n",
    "    for l in links:\n",
    "        l_lower = l.lower()  # Transformation of capital letter into lower case letter\n",
    "        if required_keywords and not any(keyword.lower() in l_lower for keyword in required_keywords):  # The function skip the url if no required keywords are in the url\n",
    "            continue\n",
    "        if domain and urlparse(l).netloc != domain:  # The function filter the links that are not in the domain\n",
    "            continue\n",
    "        if l in already_seen:  # The function filter the links already in the list of links\n",
    "            continue\n",
    "        filtered.append(l) # If the url past all the filter, it is added to the list of links\n",
    "    return filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
