{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8002b971",
   "metadata": {},
   "source": [
    "Partie 1 : Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9ee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD = c:\\Users\\Violaine\\OneDrive\\Ecole\\Master 1\\Web mining\\Projet Web Mining\\Projet_Web_Mining\\notebooks\n",
      "data exists here? -> True\n",
      "data exists one level up? -> False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "print(\"Emplacement Porjet_Web_Mining =\", os.getcwd()) # affiche le dossier depuis lequel le script est lancé\n",
    "print(\"data exists here? ->\", os.path.isdir(\"data\"))\n",
    "print(\"data exists one level up? ->\", os.path.isdir(\"../data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préparation des dossiers (évite erreurs “file not found”)\n",
    "os.chdir(\"..\")  # remonte de notebooks/ vers la racine du projet\n",
    "os.makedirs(\"data\", exist_ok=True)  # crée /data si pas déjà existant\n",
    "os.makedirs(\"notebooks\", exist_ok=True)  # crée /notebooks si pas déjà existant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a405bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des modules necessaires\n",
    "import os # gestion de dossiers/fichiers\n",
    "import re  # pour nettoyer un peu le texte (regex)\n",
    "import time  # pour temporiser les requêtes (politesse + éviter blocages)\n",
    "import random  # pour varier un peu les pauses (moins \"robot\")\n",
    "import json  # pour stocker des listes (liens) proprement dans un CSV\n",
    "from urllib.parse import urljoin, urlparse  # pour gérer URL relatives + domaines\n",
    "\n",
    "import requests  # c'est ça qui va faire les requêtes HTTP (scraping)\n",
    "from bs4 import BeautifulSoup  # pour parser le HTML\n",
    "import pandas as pd  # pour stocker et exporter nos données\n",
    "import numpy as np  # utile plus tard (matrices / graphes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c56496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètre de dépard\n",
    "HEADERS = {  # on met un User-Agent classique, sinon certains sites renvoient 403 --> on fait croire qu'on est un vrai navigateur\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/120.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "REQUEST_TIMEOUT = 15  # si le site ne répond pas en 15 sec, on abandonne (évite de bloquer tout le notebook)\n",
    "SLEEP_MIN = 1.0  # pause min entre 2 pages\n",
    "SLEEP_MAX = 2.5  # pause max entre 2 pages\n",
    "\n",
    "MAX_ARTICLES_PER_BLOG = 20  # on limite pour garder un corpus raisonnable (et pas scraper 2000 pages)\n",
    "MAX_LINKS_PER_ARTICLE = 30  # limiter liens sortants => graphe plus propre + Gephi plus lisible\n",
    "\n",
    "OUTPUT_ARTICLES = \"data/articles.csv\"  # notre corpus texte + métadonnées\n",
    "OUTPUT_GEPHI_NODES = \"data/gephi_nodes.csv\"  # noeuds pour Gephi\n",
    "OUTPUT_GEPHI_EDGES = \"data/gephi_edges.csv\"  # arêtes pour Gephi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d32e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blogs de départ (à modifier si necessaire)\n",
    "SEED_BLOGS = [\n",
    "    \"https://www.apartmenttherapy.com/\",   # lifestyle / maison\n",
    "    \"https://www.theeverygirl.com/\",       # lifestyle / self-care / carrière\n",
    "    \"https://cupcakesandcashmere.com/\",    # lifestyle / mode\n",
    "    \"https://www.goop.com/\",               # lifestyle / bien-être\n",
    "    \"https://www.thespruce.com/\",          # maison / lifestyle\n",
    "    \"https://www.mindbodygreen.com/\",      # bien-être / lifestyle\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7932aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qu'on utilise \n",
    "def polite_sleep():  # petite pause entre requêtes\n",
    "    time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))  # pause aléatoire --> pour parraitre plus humain et moin rorbot\n",
    "\n",
    "def is_valid_url(url: str) -> bool:  # filtre basique de liens inutiles\n",
    "    if not url:  # vide\n",
    "        return False\n",
    "    if url.startswith((\"mailto:\", \"tel:\", \"#\")):  # email / tel / ancre\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def normalize_url(url: str) -> str:  # normaliser un peu pour éviter doublons\n",
    "    url = url.strip()  # enlever espaces\n",
    "    url = url.split(\"#\")[0]  # enlever ancres\n",
    "    if url.endswith(\"/\"):  # enlever / final (ça évite de compter 2 fois la même page)\n",
    "        url = url[:-1]\n",
    "    return url\n",
    "\n",
    "def get_domain(url: str) -> str:  # récupérer le domaine\n",
    "    return urlparse(url).netloc.lower()\n",
    "\n",
    "def fetch_html(url: str) -> str | None:\n",
    "    # ici on met requests.get dans une fonction, c'est plus clair que de répéter 20 fois la même ligne\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)  # requête HTTP\n",
    "        if r.status_code != 200:  # si pas OK\n",
    "            return None\n",
    "        return r.text  # HTML brut\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "def extract_links(base_url: str, soup: BeautifulSoup) -> list[str]:\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):  # tous les liens <a href=\"...\">\n",
    "        href = a.get(\"href\")\n",
    "        if not is_valid_url(href):\n",
    "            continue\n",
    "        full = urljoin(base_url, href)  # transforme les liens relatifs en liens complets\n",
    "        full = normalize_url(full)\n",
    "        links.append(full)\n",
    "    return links\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # ici on fait simple : enlever multi-espaces + trim (espace de début et de fin)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_main_text(soup: BeautifulSoup) -> str:\n",
    "    # on enlève les scripts et styles car ça pollue fort le texte\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    return clean_text(soup.get_text(separator=\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a772817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reperer un article ou un blog grace a l'oeuristique\n",
    "def looks_like_article(url: str) -> bool:\n",
    "    u = url.lower()\n",
    "    patterns = [\"blog\", \"post\", \"posts\", \"article\", \"story\", \"stories\"] # mots courants dans les URL d'articles de blog\n",
    "    if any(p in u for p in patterns):\n",
    "        return True\n",
    "    if re.search(r\"/\\d{4}/\\d{2}/\", u):  # format /2025/09/ souvent utilisé dans les blogs\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d035ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping d'un blog pour extraire articles\n",
    "def collect_blog_articles(seed_url: str) -> list[dict]:  # fonction principale : récupère des articles à partir d'un blog \"seed\"\n",
    "    \n",
    "    seed_url = normalize_url(seed_url)  # on normalise l'URL (évite doublons du type / à la fin)\n",
    "    seed_domain = get_domain(seed_url)  # on récupère le domaine (ex: mindbodygreen.com)\n",
    "\n",
    "    html = fetch_html(seed_url)  # on télécharge le HTML de la page d'accueil du blog\n",
    "    if html is None:  # si on n'a pas réussi à télécharger la page\n",
    "        return []  # on renvoie une liste vide (pas d'article)\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")  # on transforme le HTML en objet BeautifulSoup (plus facile à lire)\n",
    "\n",
    "    all_links = extract_links(seed_url, soup)  # on récupère tous les liens présents sur la page d'accueil\n",
    "    internal_links = []  # on va stocker uniquement les liens internes (même domaine)\n",
    "\n",
    "    for link in all_links:  # on parcourt tous les liens trouvés\n",
    "        if get_domain(link) == seed_domain:  # si le lien appartient au même domaine que le blog\n",
    "            internal_links.append(link)  # alors c'est un lien interne, on le garde\n",
    "\n",
    "    candidate_links = []  # ici on va garder les liens qui ressemblent à des articles\n",
    "    for link in internal_links:  # on parcourt les liens internes\n",
    "        if looks_like_article(link):  # si l'URL ressemble à un article (heuristique simple)\n",
    "            candidate_links.append(link)  # on l'ajoute à la liste des candidats\n",
    "\n",
    "    # enlever les doublons, sans casser l'ordre\n",
    "    candidate_links = list(dict.fromkeys(candidate_links))  # astuce simple: dict garde l'ordre et supprime doublons\n",
    "\n",
    "    articles = []  # liste finale qui contiendra les articles récupérés (en dict)\n",
    "\n",
    "    for url in candidate_links[:MAX_ARTICLES_PER_BLOG]:  # on limite le nombre d'articles par blog (évite scrape trop large)\n",
    "        \n",
    "        polite_sleep()  # pause entre deux requêtes (évite d'être bloqué + respect des sites)\n",
    "        \n",
    "        article_html = fetch_html(url)  # on télécharge le HTML de la page candidate\n",
    "        if article_html is None:  # si la page ne répond pas ou erreur\n",
    "            continue  # on passe à l'URL suivante\n",
    "\n",
    "        article_soup = BeautifulSoup(article_html, \"html.parser\")  # parser HTML de l'article\n",
    "\n",
    "        title_tag = article_soup.find(\"title\")  # on récupère la balise <title> (souvent le titre de la page)\n",
    "        if title_tag:  # si la balise existe\n",
    "            title = title_tag.get_text(strip=True)  # on récupère le texte du titre en enlevant les espaces\n",
    "        else:  # si pas de titre\n",
    "            title = \"\"  # on met vide\n",
    "\n",
    "        text = extract_main_text(article_soup)  # on extrait le texte principal (sans scripts/styles)\n",
    "        \n",
    "        if len(text) < 500:  # si le texte est très court, c'est souvent une page \"annexe\" (menu/cookie/etc.)\n",
    "            continue  # on l'ignore\n",
    "\n",
    "        out_links = extract_links(url, article_soup)  # on récupère les liens présents dans l'article\n",
    "        out_links = out_links[:MAX_LINKS_PER_ARTICLE]  # on limite le nombre de liens sortants (sinon graphe énorme)\n",
    "\n",
    "        article_data = {  # on prépare un dictionnaire avec les infos utiles pour notre projet\n",
    "            \"seed_blog\": seed_url,  # URL du blog de départ\n",
    "            \"seed_domain\": seed_domain,  # domaine du blog (représente le blog dans le graphe)\n",
    "            \"url\": url,  # URL de l'article\n",
    "            \"title\": title,  # titre de l'article\n",
    "            \"text\": text,  # texte brut (servira pour le text mining)\n",
    "            \"out_links\": json.dumps(out_links),  # on stocke la liste en JSON pour la sauver dans un CSV\n",
    "        }  # fin dict\n",
    "\n",
    "        articles.append(article_data)  # on ajoute cet article à la liste finale\n",
    "\n",
    "    return articles  # on renvoie la liste de tous les articles récupérés pour ce blog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6981bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from: https://www.apartmenttherapy.com/\n",
      "  -> 0 articles collected\n",
      "Collecting from: https://www.theeverygirl.com/\n",
      "  -> 0 articles collected\n",
      "Collecting from: https://cupcakesandcashmere.com/\n",
      "  -> 1 articles collected\n",
      "Collecting from: https://www.goop.com/\n",
      "  -> 0 articles collected\n",
      "Collecting from: https://www.thespruce.com/\n",
      "  -> 0 articles collected\n",
      "Collecting from: https://www.mindbodygreen.com/\n",
      "  -> 15 articles collected\n",
      "Total articles: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_blog</th>\n",
       "      <th>seed_domain</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>out_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cupcakesandcashmere.com</td>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>https://cupcakesandcashmere.com/blog</td>\n",
       "      <td>Home - Cupcakes &amp; Cashmere</td>\n",
       "      <td>Home - Cupcakes &amp; Cashmere Skip to content Fas...</td>\n",
       "      <td>[\"https://www.facebook.com/cupcakesandcashmere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.mindbodygreen.com</td>\n",
       "      <td>www.mindbodygreen.com</td>\n",
       "      <td>https://www.mindbodygreen.com/articles/truth-a...</td>\n",
       "      <td>The Truth About Lean Muscle, GLP-1s &amp; Women’s ...</td>\n",
       "      <td>The Truth About Lean Muscle, GLP-1s &amp; Women’s ...</td>\n",
       "      <td>[\"https://www.mindbodygreen.com/accessibility\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.mindbodygreen.com</td>\n",
       "      <td>www.mindbodygreen.com</td>\n",
       "      <td>https://www.mindbodygreen.com/articles/altras-...</td>\n",
       "      <td>Altra’s Experience Flow 2 Is A Standout For Th...</td>\n",
       "      <td>Altra’s Experience Flow 2 Is A Standout For Th...</td>\n",
       "      <td>[\"https://www.mindbodygreen.com/accessibility\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.mindbodygreen.com</td>\n",
       "      <td>www.mindbodygreen.com</td>\n",
       "      <td>https://www.mindbodygreen.com/articles/what-to...</td>\n",
       "      <td>What The Toned Arms Aesthetic Really Means For...</td>\n",
       "      <td>What The Toned Arms Aesthetic Really Means For...</td>\n",
       "      <td>[\"https://www.mindbodygreen.com/accessibility\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.mindbodygreen.com</td>\n",
       "      <td>www.mindbodygreen.com</td>\n",
       "      <td>https://www.mindbodygreen.com/articles/these-n...</td>\n",
       "      <td>These Natural Ingredients Could Gently Support...</td>\n",
       "      <td>These Natural Ingredients Could Gently Support...</td>\n",
       "      <td>[\"https://www.mindbodygreen.com/accessibility\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         seed_blog              seed_domain  \\\n",
       "0  https://cupcakesandcashmere.com  cupcakesandcashmere.com   \n",
       "1    https://www.mindbodygreen.com    www.mindbodygreen.com   \n",
       "2    https://www.mindbodygreen.com    www.mindbodygreen.com   \n",
       "3    https://www.mindbodygreen.com    www.mindbodygreen.com   \n",
       "4    https://www.mindbodygreen.com    www.mindbodygreen.com   \n",
       "\n",
       "                                                 url  \\\n",
       "0               https://cupcakesandcashmere.com/blog   \n",
       "1  https://www.mindbodygreen.com/articles/truth-a...   \n",
       "2  https://www.mindbodygreen.com/articles/altras-...   \n",
       "3  https://www.mindbodygreen.com/articles/what-to...   \n",
       "4  https://www.mindbodygreen.com/articles/these-n...   \n",
       "\n",
       "                                               title  \\\n",
       "0                         Home - Cupcakes & Cashmere   \n",
       "1  The Truth About Lean Muscle, GLP-1s & Women’s ...   \n",
       "2  Altra’s Experience Flow 2 Is A Standout For Th...   \n",
       "3  What The Toned Arms Aesthetic Really Means For...   \n",
       "4  These Natural Ingredients Could Gently Support...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Home - Cupcakes & Cashmere Skip to content Fas...   \n",
       "1  The Truth About Lean Muscle, GLP-1s & Women’s ...   \n",
       "2  Altra’s Experience Flow 2 Is A Standout For Th...   \n",
       "3  What The Toned Arms Aesthetic Really Means For...   \n",
       "4  These Natural Ingredients Could Gently Support...   \n",
       "\n",
       "                                           out_links  \n",
       "0  [\"https://www.facebook.com/cupcakesandcashmere...  \n",
       "1  [\"https://www.mindbodygreen.com/accessibility\"...  \n",
       "2  [\"https://www.mindbodygreen.com/accessibility\"...  \n",
       "3  [\"https://www.mindbodygreen.com/accessibility\"...  \n",
       "4  [\"https://www.mindbodygreen.com/accessibility\"...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lancer la collecte sur tout les blogs\n",
    "all_articles = []  # liste globale où on mettra tous les articles de tous les blogs\n",
    "\n",
    "for seed in SEED_BLOGS:  # on parcourt chaque blog seed\n",
    "    \n",
    "    print(\"Collecting from:\", seed)  # petit affichage pour suivre l'avancement\n",
    "    \n",
    "    data = collect_blog_articles(seed)  # on récupère les articles de ce blog\n",
    "    \n",
    "    print(\"  ->\", len(data), \"articles collected\")  # on affiche combien d'articles on a réussi à récupérer\n",
    "    \n",
    "    all_articles.extend(data)  # on ajoute tous les articles collectés à la liste globale\n",
    "\n",
    "df_articles = pd.DataFrame(all_articles)  # transformer la liste de dictionnaires en DataFrame pandas\n",
    "\n",
    "df_articles.drop_duplicates(subset=[\"url\"], inplace=True)  # enlever les doublons d'URL si jamais\n",
    "\n",
    "print(\"Total articles:\", len(df_articles))  # afficher le total final d'articles collectés\n",
    "\n",
    "df_articles.head(5)  # afficher un aperçu des 5 premières lignes (utile pour vérifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16f839bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/articles.csv\n"
     ]
    }
   ],
   "source": [
    "# sauvegarde du corpus en CSV\n",
    "df_articles.to_csv(OUTPUT_ARTICLES, index=False, encoding=\"utf-8\")  # on sauve le corpus, sans la colonne index\n",
    "print(\"Saved:\", OUTPUT_ARTICLES)  # confirmation dans la console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efdbb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/gephi_nodes.csv\n",
      "Saved: data/gephi_edges.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>www.facebook.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>www.instagram.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>pinterest.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>www.youtube.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>www.bloglovin.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>cupcakesandcashmere.com</td>\n",
       "      <td>Directed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Source                   Target      Type\n",
       "0  cupcakesandcashmere.com         www.facebook.com  Directed\n",
       "1  cupcakesandcashmere.com              twitter.com  Directed\n",
       "2  cupcakesandcashmere.com        www.instagram.com  Directed\n",
       "3  cupcakesandcashmere.com            pinterest.com  Directed\n",
       "4  cupcakesandcashmere.com          www.youtube.com  Directed\n",
       "5  cupcakesandcashmere.com        www.bloglovin.com  Directed\n",
       "6  cupcakesandcashmere.com  cupcakesandcashmere.com  Directed\n",
       "7  cupcakesandcashmere.com  cupcakesandcashmere.com  Directed\n",
       "8  cupcakesandcashmere.com  cupcakesandcashmere.com  Directed\n",
       "9  cupcakesandcashmere.com  cupcakesandcashmere.com  Directed"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construire fichiers Gephi (nodes + edges)\n",
    "nodes = set()  # set = évite les doublons automatiquement (chaque noeud une seule fois)\n",
    "edges = []  # liste des liens (arêtes) entre noeuds\n",
    "\n",
    "for _, row in df_articles.iterrows():  # on parcourt chaque ligne (article) du DataFrame\n",
    "    \n",
    "    source = row[\"seed_domain\"]  # le noeud \"source\" = domaine du blog d'origine\n",
    "    \n",
    "    nodes.add(source)  # on ajoute la source dans la liste des noeuds\n",
    "    \n",
    "    out_links = json.loads(row[\"out_links\"])  # on re-transforme la chaîne JSON en vraie liste python\n",
    "    \n",
    "    for link in out_links:  # pour chaque lien sortant trouvé dans l'article\n",
    "        \n",
    "        target = get_domain(link)  # le noeud \"cible\" = domaine du lien sortant\n",
    "        \n",
    "        if target == \"\":  # si le domaine est vide (rare mais possible)\n",
    "            continue  # on ignore\n",
    "        \n",
    "        nodes.add(target)  # on ajoute aussi le noeud cible\n",
    "        \n",
    "        edge = {  # on crée une arête Gephi\n",
    "            \"Source\": source,  # colonne Source attendue par Gephi\n",
    "            \"Target\": target,  # colonne Target attendue par Gephi\n",
    "            \"Type\": \"Directed\",  # graphe dirigé (utile pour PageRank / HITS)\n",
    "        }  # fin dict\n",
    "        \n",
    "        edges.append(edge)  # on ajoute l'arête à la liste\n",
    "\n",
    "df_nodes = pd.DataFrame(sorted(list(nodes)), columns=[\"Id\"])  # Gephi aime une colonne \"Id\" pour les noeuds\n",
    "df_nodes[\"Label\"] = df_nodes[\"Id\"]  # on met Label = Id pour simplifier\n",
    "\n",
    "df_edges = pd.DataFrame(edges)  # on transforme la liste d'arêtes en DataFrame\n",
    "\n",
    "df_nodes.to_csv(OUTPUT_GEPHI_NODES, index=False, encoding=\"utf-8\")  # export nodes\n",
    "df_edges.to_csv(OUTPUT_GEPHI_EDGES, index=False, encoding=\"utf-8\")  # export edges\n",
    "\n",
    "print(\"Saved:\", OUTPUT_GEPHI_NODES)  # confirmation\n",
    "print(\"Saved:\", OUTPUT_GEPHI_EDGES)  # confirmation\n",
    "\n",
    "df_edges.head(10)  # aperçu de quelques liens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d813569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de blogs (seeds) : 6\n",
      "Nb d'articles : 16\n",
      "Nb de noeuds (domaines) : 26\n",
      "Nb d'arêtes (liens) : 480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "seed_domain\n",
       "www.mindbodygreen.com      15\n",
       "cupcakesandcashmere.com     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# controle qualité (on verifie si scraping ok)\n",
    "print(\"Nb de blogs (seeds) :\", len(SEED_BLOGS))  # combien de blogs on a essayé de scraper\n",
    "print(\"Nb d'articles :\", len(df_articles))  # taille du corpus final\n",
    "print(\"Nb de noeuds (domaines) :\", len(df_nodes))  # combien de domaines dans le graphe\n",
    "print(\"Nb d'arêtes (liens) :\", len(df_edges))  # combien de liens dans le graphe\n",
    "\n",
    "# Voir quels blogs ont fourni le plus d'articles (ça aide pour voir si un blog bug)\n",
    "df_articles[\"seed_domain\"].value_counts()  # tri automatique (du plus fréquent au moins fréquent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28b6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML downloaded: False\n",
      "Domain: www.apartmenttherapy.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#debug si un blog ne marche pas\n",
    "test_seed = SEED_BLOGS[0]  # on prend le premier seed\n",
    "html = fetch_html(test_seed)  # on télécharge la page d'accueil\n",
    "print(\"HTML downloaded:\", html is not None)  # vérifier si on a bien du HTML\n",
    "print(\"Domain:\", get_domain(test_seed))  # vérifier le domaine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
