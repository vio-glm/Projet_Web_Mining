{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCHEMA MEtHODO : SCRAPPING, TEXT MINING, TF-IDF, LDA, BERTopic, COMPARAISON LDA/BERTopic, LINK ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efca131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##fltrer pour garder 'les vrais documents'## --> au cas ou on a trop il faut encore filtrer sinon on ne sait pas utiliser ( avoir avec tt les liens)\n",
    "df_final[\"word_count\"] = df_final[\"cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "df_articles = df_final[df_final[\"word_count\"] >= 400].copy()  # seuil à ajuster au cas ou pour enlever le bruit\n",
    "print(\"Docs gardés:\", len(df_articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectoriser (Bag-of-Words) pour LDA\n",
    "'''from sklearn.feature_extraction.text import CountVectorizer''' #--> déja ds le code d eury mais utilisé ici\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,      # mot présent dans au moins 5 docs ####--> a modifier en focntion du nombre d'article\n",
    "    max_df=0.8     # ignore les mots trop fréquents ####--> a modifier en focntion du nombre d'article\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df_articles[\"cleaned\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "\n",
    "n_topics = 6  # bon point de départ pour lifestyle # a modifier au cas ou en fonction de tout nos liens --> si 2 mot topique trop similaire nombre trop grand\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42\n",
    ")\n",
    "lda.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562faa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## afficher les topics (top mots)\n",
    "\n",
    "'''import numpy as np ''' #--> déja ds le code d eury mais utilisé ici\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out() # Récupérer la liste des mots du vocabulaire appris par le vectorizer\n",
    "\n",
    "def show_topics(model, feature_names, n_top_words=10): \n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):# Parcourir chaque topic appris par le modèle\n",
    "                                        # model.components_ = matrice (topics x mots)\n",
    "        top_idx = topic.argsort()[:-n_top_words-1:-1] # Trier les mots du topic par importance décroissante \n",
    "                        # argsort() donne les indices triés → on prend les plus grands poids\n",
    "        top_words = [feature_names[i] for i in top_idx] # Convertir les indices en mots lisibles\n",
    "        topics.append((topic_idx, top_words))  # Sauvegarder le topic\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\") # affichage lisible du topic\n",
    "    return topics\n",
    "\n",
    "topics = show_topics(lda, feature_names, n_top_words=12) #afficher les 12 mots les plus importants pour chaque topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4748e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic dominant par documents (plus ex url)\n",
    "\n",
    "topic_dist = lda.transform(X) # Calculer la distribution des topics pour chaque document --> une matrice (documents x topics)\n",
    "df_articles[\"dominant_topic\"] = topic_dist.argmax(axis=1) # Pour chaque article, on sélectionne le topic le plus dominant (celui avec la probabilité la plus élevée)\n",
    "\n",
    "for t in range(n_topics):\n",
    "    print(f\"\\n=== Topic {t} ===\")\n",
    "    print(\"Exemples d'URLs:\")\n",
    "    print(df_articles[df_articles[\"dominant_topic\"] == t][\"url\"].head(5).to_list())\n",
    "    # --> # Sélectionner quelques articles dont le topic dominant est t\n",
    "    # head(5) permet d'afficher seulement quelques exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165edfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Répartition des topics\n",
    "\n",
    "# Compter le nombre d'articles associés à chaque topic dominant\n",
    "# value_counts() → nombre de documents par topic\n",
    "# sort_index() → afficher les topics dans l'ordre (0, 1, 2, ...)\n",
    "topic_counts = df_articles[\"dominant_topic\"].value_counts().sort_index()\n",
    "print(topic_counts)\n",
    "print(\"Pourcentages:\", (topic_counts / topic_counts.sum()).round(3)) # Calcul de la proportion (en %) de chaque topic dans le corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e783657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Semantic Analysis – BERTopic (single blog: MindBodyGreen)\n",
    "# Each document = one blog article\n",
    "# Goal: discover latent themes and compare with LDA results\n",
    "# ============================================================\n",
    "\n",
    "'''from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer''' #--> déja ds le code d eury mais utilisé ici\n",
    "\n",
    "BLOG_FOR_BERT = \"mindbodygreen\"  # <-- only this blog will be analyzed with BERTopic\n",
    "\n",
    "# 1) Keep only documents from ONE blog\n",
    "df_bertopic = df_final[df_final[\"blog_name\"] == BLOG_FOR_BERT].copy()\n",
    "\n",
    "# 2) Keep only real articles (remove category/tag/short pages)\n",
    "df_bertopic[\"word_count\"] = df_bertopic[\"cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "df_bertopic = df_bertopic[df_bertopic[\"word_count\"] >= 400].copy()\n",
    "\n",
    "# List of documents for BERTopic\n",
    "docs = df_bertopic[\"cleaned\"].astype(str).tolist()\n",
    "\n",
    "print(f\"BERTopic applied on blog: {BLOG_FOR_BERT}\")\n",
    "print(\"Number of documents used for BERTopic:\", len(docs))\n",
    "\n",
    "# 3) Load sentence embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4) Initialize BERTopic\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    min_topic_size=20,\n",
    "    nr_topics=\"auto\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5) Fit the model and assign a topic to each document\n",
    "topics, probs = topic_model.fit_transform(docs, embedding_model=embedding_model)\n",
    "\n",
    "# 6) Attach topic labels to the dataframe\n",
    "df_bertopic[\"bertopic_topic\"] = topics\n",
    "\n",
    "# 7) Display main topics and representative keywords\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"\\nTop discovered topics:\")\n",
    "display(topic_info.head(10))\n",
    "\n",
    "# 8) Show keywords and example URLs for each topic\n",
    "for t in topic_info[\"Topic\"].tolist():\n",
    "    if t == -1:\n",
    "        continue\n",
    "    print(f\"\\n=== Topic {t} ===\")\n",
    "    print(\"Top keywords:\", [w for w, _ in topic_model.get_topic(t)[:10]])\n",
    "    print(\"Example URLs:\")\n",
    "    print(df_bertopic[df_bertopic[\"bertopic_topic\"] == t][\"url\"].head(3).to_list())\n",
    "\n",
    "# 9) Summary of topic distribution --> tables easy to compare with LDA\n",
    "bertopic_summary = df_bertopic[\"bertopic_topic\"].value_counts().reset_index()\n",
    "bertopic_summary.columns = [\"topic\", \"n_docs\"]\n",
    "display(bertopic_summary.head(10))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
